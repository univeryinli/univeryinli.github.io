<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习知识点汇总]]></title>
    <url>%2F2019%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。下面分机器学习和深度学习分开理清楚知识点 机器学习最近在开始找工作，顺便复习一下之前学习的机器学习知识点，从而对机器学习和深度方面有一个比较系统的了解，还有机器学习和深度学习优化过程中出现的技术。推荐的博客：https://www.cnblogs.com/jerrylead/tag/Machine%20Learning/ 该博客列表如下：偏最小二乘法回归（Partial Least Squares Regression）典型关联分析（Canonical Correlation Analysis增强学习（Reinforcement Learning and Control因子分析（Factor Analysis）线性判别分析（Linear Discriminant Analysis）（一)线性判别分析（Linear Discriminant Analysis）（二）ICA扩展描述独立成分分析（Independent Component Analysis）主成分分析（Principal components analysis）-最小平方误差解释 主成分分析（Principal components analysis）-最大方差解释主成分分析（Principal components analysis）-最大方差解释在线学习（Online Learning）（EM算法）The EM Algorithm混合高斯模型（Mixtures of Gaussians）和EM算法K-means聚类算法规则化和模型选择（Regularization and model selection）支持向量机SVM（一）支持向量机SVM（二）支持向量机（三）核函数支持向量机（四）支持向量机（五）SMO算法判别模型、生成模型与朴素贝叶斯方法对线性回归，logistic回归和一般回归的认识 机器学习知识点补充： 参见链接：https://github.com/HuiZhou-xmu/Machine-Learning-Deep-Learning-Summary朴素贝叶斯方法决策树随机森林bagging和boost算法adboost算法激活函数过拟合 贝叶斯定理以及贝叶斯网络参考博客：https://blog.csdn.net/v_JULY_v/article/details/40984699 机器学习优化方法 参考博客：https://zhuanlan.zhihu.com/p/22252270 深度学习半监督学习可以参考：半监督学习方法—%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.md) 无监督学习参考邱锡鹏教授第九章书籍:神经网络与深度学习第九章 强化学习参考邱锡鹏教授第十四章书籍:神经网络与深度学习第十四章 迁移学习参考机器之心博客：迁移学习当然迁移各家都有自己的看法，这一部分没有固定的知识点，很灵活，最终的目的都是要最大可能的优化和提升模型的计算速度和准确度 概率图模型参考邱锡鹏教授第十一章书籍:神经网络与深度学习第十一章 注意本博客主要是对现在网络上的机器学习和深度学习以及相关的优化方法进行系统性的汇总，从而让自己的知识形成体系，原则上自己只书写少量知识点，主要内容来自其他网络作者，相当于文献中的综述吧，也希望能够给别人带来方便。 推荐文章（由hexo文章推荐插件驱动）豆瓣小组 与 机器学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“人工智能”前景：只有人工，没有智能]]></title>
    <url>%2F2019%2F07%2F28%2F%E2%80%9C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E2%80%9D%E5%89%8D%E6%99%AF%EF%BC%9A%E5%8F%AA%E6%9C%89%E4%BA%BA%E5%B7%A5%EF%BC%8C%E6%B2%A1%E6%9C%89%E6%99%BA%E8%83%BD%2F</url>
    <content type="text"><![CDATA[身处在人工智能时代的我们是很幸运的，我们可以随时享受科技给我们带来的便利生活，这其中就有人工智能给我们的生活和工作带来的便利，我们可以在早上出门的时候，可以用一个手机就可以直接找到附近的出租车，接着出租车就会来到我们的门前将我们载到我们去的地方，而且出租车也不需要人的参与就可以实现完全的自动驾驶，也就是平常我们所说的无人驾驶L5级别，然后可以坐着无人驾驶汽车行驶到目的地，进入自己工作的地方以后，手机上面自己定制的早餐早就在办公室的餐口等候了，只要来的时候通过人脸识别技术扫描就可以零取自己的餐食，然后坐下来办公以后，智能机器人就送来了今天需要处理的文件和数据，并且给机器人发送指令，看起来工作是多么的智能化。 这一套智能的流程里面，仿人智能是一个重要的因素，仿人智能是人工智能的高级形态，也就是说用计算机来模仿人的智能，用计算机来模仿人的思维能力和推算、以及逻辑推演能力，这种也是人工智能的高级形态，目前人工智能的水平可以是说还处于比较低级的阶段，应该是处于细胞阶段，基本没有人的智能，只能在某一方专业领域方面做到极致，比如图像识别领域的人脸识别。 以目前人工智能发展的趋势来看，真正的仿人智能式的人工智能离我们还很遥远。 计算能力限制了人工智能的发展，现在一般的深度学习算法也是需要高性能计算机的处理才能有一定实时处理能力，就大家熟悉的人脸识别这项科技来看，实时的人脸识别算法的背后是大规模服务器集群在发挥着作用，服务器集群为人脸识别提供了保证，但是人脸识别只是仿人智能中一个很小的功能而已，而且他跟我们人眼看到的客观世界是两个概念，我们人眼看到的世界要处理很多信息，比如分析我们看到的物体是什么保留有用的，剔除无用的，并且人眼还要判定物体的远近程度，这对于神经网络来说是一项巨大的工程，因为人脸识别网络只是做一件事情，不断判定别人的脸信息，至于图像中出现的其他物体全然不知，从而也无法做出更加深入的判断，单就人眼需要接受到的信息来看，与简单的人脸识别网络，这还有很长的路要走，计算能力和计算容量是关键的因素。 算法的复杂度也是一个限制因素，假如我们真的具备多物体的快速检测和定位以及信息提取功能，那么如何将我们人眼实时接受的多物体之间的信息融合起来，这个是一个非常复杂的算法问题，对于这一点，脑科学现在也没有办法将这个问题解释得清楚，所以人类也不可能在短期内将自身人脑的复杂度复现出来，即使脑科学已经将人类这部分信息融合的方法的奥秘探索出来了，但是要想实现这么一个复杂的算法，我们现在的所用的程序语言支持不支持就不知道了，多物体的信息融合并且同时还有可能伴随着信息压缩，然后才会给大脑的中枢做进一步的处理，所以这又给仿人智能提出了一道新的难题，因为人脑这么高级的组合不可能只用一种信息压缩的方式。 大脑的复杂度，是我们很难模仿的，众所周知，大脑有很高的复杂度，神经元的数量数以亿计，而且神经元的组合方式远比我们想像的要复杂的多，神经元之间的连接量级也比我们想象的复杂的多，很有可能神经元的链接有着很复杂的信息传递方式，同一个神经元输入同一个值以后就可能会出现不同的输出值，而且很有可能这不是一个确定值，可能是一个动态值，这个动态值的一定的范围代表某种操作，这个在我们严密的数学逻辑里面是没有办法解释的，可能只有等复杂度上升到一定的程度以后才有可能认清楚这个机理，但是以目前的计算机发展的科学来看，还是有很远的距离的。 计算机科学本身可能就对仿人智能有着本质的阻碍，现在的计算机是二进制计算机，对于晶体管的状态只能是开关两种状态，所以导致其计算能力有所限制，这会在根本上限制计算机的能力，所以晶体管的多状态就会提升计算机的能力，这就让仿人智能成为可能。 无人驾驶汽车是现在人工智能方向热门方向，很多大公司也都盯着这个项目，指着这个项目能够有所进展，最终制造出来无人机驾驶汽车，但是根据目前人工智能技术的发展，要做到无人驾驶的L5级别还是有一定距离的，路面情况太过复杂，多传感器融合的技术尚没有很好的解决方案，做决策的时候都会出现一些比较片面的问题。再加上计算机处理能力的限制，真正意义上的无人汽车还比较遥远，所以，公众对待无人汽车这个事情，要有一定理性思维，现在的很多商人因为需要融资，或者圈钱，卖股票，都在吹嘘人工智能如何如何有用，如何如何强势，能够像电影里面的智能机器人一样，能够做很多人类比较复杂的事情。 之所以写这篇文章，也就是想给大家一个认识，告诉大家现在的人工智能技术的进展，我们对待人工智能技术要有一定的理性的认识。]]></content>
      <categories>
        <category>News</category>
      </categories>
      <tags>
        <tag>人工智能前景</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux解压缩指令]]></title>
    <url>%2F2019%2F07%2F28%2Flinux%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[成功只是比失败多走了一步而已，坚持自己的梦想，永不言弃! .tar解包：tar xvf FileName.tar打包：tar cvf FileName.tar DirName（注：tar是打包，不是压缩！）——————————————— .gz解压1：gunzip FileName.gz解压2：gzip -d FileName.gz压缩：gzip FileName .tar.gz 和 .tgz解压：tar zxvf FileName.tar.gz压缩：tar zcvf FileName.tar.gz DirName——————————————— .bz2解压1：bzip2 -d FileName.bz2解压2：bunzip2 FileName.bz2压缩： bzip2 -z FileName .tar.bz2解压：tar jxvf FileName.tar.bz2压缩：tar jcvf FileName.tar.bz2 DirName——————————————— .bz解压1：bzip2 -d FileName.bz解压2：bunzip2 FileName.bz压缩：未知 .tar.bz解压：tar jxvf FileName.tar.bz压缩：未知——————————————— .Z解压：uncompress FileName.Z压缩：compress FileName .tar.Z解压：tar Zxvf FileName.tar.Z压缩：tar Zcvf FileName.tar.Z DirName——————————————— .zip解压：unzip FileName.zip压缩：zip FileName.zip DirName——————————————— .rar解压：rar x FileName.rar压缩：rar a FileName.rar DirName——————————————— .lha解压：lha -e FileName.lha压缩：lha -a FileName.lha FileName——————————————— .rpm解包：rpm2cpio FileName.rpm | cpio -div——————————————— .deb解包：ar p FileName.deb data.tar.gz | tar zxf -———————————————.tar .tgz .tar.gz .tar.Z .tar.bz .tar.bz2 .zip .cpio .rpm .deb .slp .arj .rar .ace .lha .lzh .lzx .lzs .arc .sda .sfx .lnx .zoo .cab .kar .cpt .pit .sit .sea解压：sEx x FileName.压缩：sEx a FileName. FileName sEx只是调用相关程序，本身并无压缩、解压功能，请注意！ gzip 命令减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。gzip 是在 Linux 系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。 语法：gzip [选项] 压缩（解压缩）的文件名该命令的各选项含义如下： -c 将输出写到标准输出上，并保留原有文件。-d 将压缩文件解压。-l 对每个压缩文件，显示下列字段： 压缩文件的大小；未压缩文件的大小；压缩比；未压缩文件的名字-r 递归式地查找指定目录并压缩其中的所有文件或者是解压缩。-t 测试，检查压缩文件是否完整。-v 对每一个压缩和解压的文件，显示文件名和压缩比。-num 用指定的数字 num 调整压缩的速度，-1 或 —fast 表示最快压缩方法（低压缩比），-9 或—best表示最慢压缩方法（高压缩比）。系统缺省值为 6。指令实例： gzip % 把当前目录下的每个文件压缩成 .gz 文件。gzip -dv % 把当前目录下每个压缩的文件解压，并列出详细的信息。gzip -l *% 详细显示例1中每个压缩的文件的信息，并不解压。gzip usr.tar% 压缩 tar 备份文件 usr.tar，此时压缩文件的扩展名为.tar.gz。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux解压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NC编号对应染色体-CHROM]]></title>
    <url>%2F2019%2F07%2F25%2FNC%E7%BC%96%E5%8F%B7%E5%AF%B9%E5%BA%94%E6%9F%93%E8%89%B2%E4%BD%93-CHROM%2F</url>
    <content type="text"><![CDATA[DescriptionNC编号中的点后面代表版本，可能会更新。 NC编号与对应的染色体 NC编号 染色体 NC_000001.10 Chr1 NC_000002.11 Chr2 NC_000003.11 Chr3 NC_000004.11 Chr4 NC_000005.9 Chr5 NC_000006.11 Chr6 NC_000007.13 Chr7 NC_000008.10 Chr8 NC_000009.11 Chr9 NC_000010.10 Chr10 NC_000011.9 Chr11 NC_000012.11 Chr12 NC_000013.10 Chr13 NC_000014.8 Chr14 NC_000015.9 Chr15 NC_000016.9 Chr16 NC_000017.10 Chr17 NC_000018.9 Chr18 NC_000019.9 Chr19 NC_000020.10 Chr20 NC_000021.8 Chr21 NC_000022.10 Chr22 NC_000023.10 ChrX NC_000024.9 ChrY NC_012920.1 ChrM]]></content>
      <categories>
        <category>Gene</category>
      </categories>
      <tags>
        <tag>染色体常识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[google scholar vs baidu scholar-what is your problem]]></title>
    <url>%2F2019%2F07%2F19%2Fproblem%2F</url>
    <content type="text"><![CDATA[Description 在AI前进的道路上会有各种各样的事情发生，但是前进的决心不会改变，AI会改变每一个人的生活。 百度学术百度学术的搜索做得跟三岁的小孩子一样的智商，具体看下图吧，说多了都是泪 谷歌学术谷歌学术对于学术这个业务是及其的人性化，我瞬间就能理解，中国人为什么能在讲座上倒水了，是一种无奈而又愤怒的最真实的表达 期望希望中国能够出现其他能够与百度搜索竞争的公司，这样可能会改变这个局面 推荐文章（由hexo文章推荐插件驱动）Hexo插件之百度主动提交链接]]></content>
      <categories>
        <category>News</category>
      </categories>
      <tags>
        <tag>news</tag>
        <tag>百度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1000 Genomes]]></title>
    <url>%2F2019%2F07%2F18%2F1000-genomes%2F</url>
    <content type="text"><![CDATA[1000 Genomes 简介1000 Genomes Project（缩写为1KGP）于2008年1月启动，是一项国际研究工作，旨在建立迄今为止最详细的人类遗传变异目录。科学家计划在接下来的三年内使用新开发的技术对来自不同种族群体的至少一千名匿名参与者的基因组进行测序，这些技术更快，更便宜。 2010年，该项目完成了试验阶段，在“自然”杂志的一篇出版物中对此进行了详细描述。2012年，1092个基因组的测序在Nature出版物中公布。 2015年，“自然”杂志上的两篇论文报告了结果，项目的完成以及未来研究的机会。确定了许多罕见的变异，仅限于密切相关的群体，并分析了8个结构变异类别。 该项目将来自世界各地研究所的多学科研究团队联合起来，包括中国，意大利，日本，肯尼亚，尼日利亚，秘鲁，英国和美国。每一个都将为庞大的序列数据集和精细的人类基因组图谱做出贡献，这些图谱将通过公共数据库免费提供给科学界和公众。 1000 Genome Project 的目标是发现在人群中频率大于1%的变异位点，对来自不同人群的大量样本进行测序，识别到了许多的变异位点，为人类遗传变异的研究提供了一个综合的资源。 网址：http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/ 人类基因组由大约30亿个DNA碱基对组成，估计携带约20,000个蛋白质编码基因。在设计研究时，该联盟需要解决有关项目指标的若干关键问题，如技术挑战，数据质量标准和序列覆盖。 整个项目划分为四个阶段，试点阶段和三个主要阶段。 为了确定整个项目的最终设计，设计了三个试点研究，并将在项目的第一年内进行 第一个试点旨在对低覆盖率（2x）的3个主要地理群体的180个人进行基因分型。 第二项初步研究，两个核心家族（父母和成年子女）的基因组将进行深度覆盖（每个基因组20倍）的测序。 第三项试点研究涉及对1000名深度覆盖（20x）的1000个基因的编码区（外显子）进行测序。 主要阶段中只有第一阶段和第三阶段产生了数据，结果发现，平均而言，每个人在注释基因中携带约250-300个功能丧失变体，并且先前涉及遗传性疾病的50-100个变体。 来自4个群体的180个个体的低覆盖度全基因组测序 2个三人组（母亲 - 孩子）的高覆盖率排序 来自7个群体的697个个体的外显子靶向测序 整个项目从2008年开始到2013年结束，最终的版本为2013年5月2日发布的数据, 包含了来自26个人群，共2504个样本的SNP分型结果。根据Fort Lauderdale principles原则，所有基因组序列数据（包括变体调用）随着项目的进展免费提供，1000G的数据是免费公开的，可以通过ftp下载得到。 1000 Genomes 数据库结构-ftp数据存储在ftp上，其目录结构如下图，包含多个说明文件*.md： README_ebi_aspera_info.md—高速下载说明 README_file_formats_and_descriptions.md—文件格式和描述 README_ftp_site_structure.md—ftp网站文件结构 README_missing_files.md—缺失文件 README_populations.md—人群 README_using_1000genomes_cram.md—cram文件读取 data_collections—数据存储的文件夹 README_ebi_aspera_info.md—高速下载说明 How to download files using AsperaDownload AsperaAspera provides a fast method of downloading data. To use the Aspera service you need to download the Aspera connect software. This provides a bulk download client called ascp. BrowserOur aspera browser interace no longer works. If you wish to download files using a web interface we recommend using the Globus interface we present. If you are previously relied on the aspera web interface and wish to discuss the matter please email us at info@1000genomes.org to discuss your options. Command lineFor the command line tool ascp, for versions 3.3.3 and newer, you need to use a command line like: 12&gt; ascp -i bin/aspera/etc/asperaweb_id_dsa.openssh -Tr -Q -l 100M -P33001 -L- fasp-g1k@fasp.1000genomes.ebi.ac.uk:vol1/ftp/release/20100804/ALL.2of4intersection.20100804.genotypes.vcf.gz ./&gt; &gt; For versions 3.3.2 and older, you need to use a command line like: 12&gt; ascp -i bin/aspera/etc/asperaweb_id_dsa.putty -Tr -Q -l 100M -P33001 -L- fasp-g1k@fasp.1000genomes.ebi.ac.uk:vol1/ftp/release/20100804/ALL.2of4intersection.20100804.genotypes.vcf.gz ./&gt; &gt; Note, the only change between these commands is that for newer versions of ascp asperaweb_id_dsa.openssh replaces asperaweb_id_dsa.putty. This change is noted by Aspera here. You can check the version of ascp you have using: 12&gt; ascp --version&gt; &gt; The argument to -i may also be different depending on the location of the default key file. The command should not ask you for a password. All the IGSR data is accessible without a password but you do need to give ascp the ssh key to complete the command. For the above commands to work with your network’s firewall you need to open ports 22/tcp (outgoing) and 33001/udp (both incoming and outgoing) to the following EBI IPs: 193.62.192.6 193.62.193.6 193.62.193.135 If the firewall has UDP flood protection, it must be turned off for port 33001. Further detailsFor further information, please contact info@1000genomes.org. README_file_formats_and_descriptions.md—文件格式和描述 File formats and descriptionsThis file provides information on some of the file formats used to make data available on this site. CRAM.cram files use a reference-based format to store data. This format is being used to supply alignment data on this site. Detailed information on working with our .cram files is provided in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/README_using_1000genomes_cram.md. CRAI.crai files are index files which accompany CRAM files. They must be present in order to work with .cram files for most purposes. BAS.bas files are .cram or .bam statistic files with one line per readgroup and columns separated bytabs. The first line is a header that describes each column. The first six columnsprovide meta information about each readgroup. The remaining columns provide various statistics about the readgroup, calculatedby going through the release bams. Where data isn’t available to calculate theresult for a column, the default value will be 0. Each column is described in detail below: 12345678910111213141516171819202122&gt; Column 1 &apos;bam_filename&apos;: The DCC bam file name in which the readgroup data can be found.&gt; Column 2 &apos;md5&apos;: The md5 checksum of the bam file named in column 1.&gt; Column 3 &apos;study&apos;: The SRA study id this readgroup belongs to.&gt; Column 4 &apos;sample&apos;: The sample (individual) identifier the readgroup came from.&gt; Column 5 &apos;platform&apos;: The sequencing platform (technology) used to sequence the readgroup.&gt; Column 6 &apos;library&apos;: The name of the library used for the readgroup.&gt; Column 7 &apos;readgroup&apos;: The readgroup identifier. This is unique per .bas file. The remaining columns summarise data for reads with this RG tag in the bam file given in column 1.&gt; Column 8 &apos;#_total_bases&apos;: The sum of the length of all reads in this readgroup.&gt; Column 9 &apos;#_mapped_bases&apos;: The sum of the length of all reads in this readgroup that did not have flag 4 (== unmapped).&gt; Column 10 &apos;#_total_reads&apos;: The total number of reads in this readgroup.&gt; Column 11 &apos;#_mapped_reads&apos;: The total number of reads in this readgroup that did not have flag 4 (== unmapped).&gt; Column 12 &apos;#_mapped_reads_paired_in_sequencing&apos;: As for column 10, but also requiring flag 1 (== reads paired in sequecing).&gt; Column 13 &apos;#_mapped_reads_properly_paired&apos;: As for column 10, but also requiring flag 2 (== mapped in a proper pair, inferred during alignment).&gt; Column 14 &apos;%_of_mismatched_bases&apos;: Calculated by summing the read lengths of all reads in this readgroup that have an NM tag, summing the edit distances obtained from the NM tags, and getting the percentage of the latter out of the former to 2 decimal places.&gt; Column 15 &apos;average_quality_of_mapped_bases&apos;: The mean of all the base qualities of the bases counted for column 8, to 2 decimal places.&gt; Column 16 &apos;mean_insert_size&apos;: The mean of all insert sizes (ISIZE field) greater than 0 for properly paired reads (as counted in column 12) and with a mapping quality (MAPQ field) greater than 0. Rounded to the nearest whole number.&gt; Column 17 &apos;insert_size_sd&apos;: The standard deviation from the mean of insert sizes considered for column 15. To 2 decimal places.&gt; Column 18 &apos;median_insert_size&apos;: The median insert size, using the same set of insert sizes considered for column 15.&gt; Column 19 &apos;insert_size_median_absolute_deviation&apos;: The median absolute deviation of the column 17 data.&gt; Column 20 &apos;#_duplicate_reads&apos;: The number of reads which were marked as duplicates.&gt; Column 21 &apos;#_duplicate_bases&apos;: The number of bases which were narked as duplicated&gt; &gt; INDEXVarious types of index file exist on the site. These are tab-delimited files where the data is arranged in columns. Immediately before the body of the file there is a header line, which starts with #, that gives the column names. In addition, index files may have further information at the head of the file. These lines start with ## and can provide descriptions of the columns, the date the index was generated and other pieces of information, as appropriate to the file and data set. An example of the start of such a file, in this case an alignment index file, is below: 123456789101112&gt; ##FileDate=20150914&gt; ##Project=Illumina Platinum pedigree&gt; ##CRAM_FILE=Path to CRAM file - information on CRAM files can be found in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/README_using_1000genomes_cram.md and information on the alignment process accompanies the data collection&gt; ##CRAM_MD5=md5 for CRAM file&gt; ##CRAI_FILE=Path to CRAI file&gt; ##CRAI_MD5=md5 for CRAI file&gt; ##BAS_FILE=Path to BAS file&gt; ##BAS_MD5=md5 for BAS file&gt; #CRAM_FILE CRAM_MD5 CRAI_FILE CRAI_MD5 BAS_FILE BAS_MD5&gt; ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/illumina_platinum_pedigree/data/CEU/NA12893/alignment/NA12893.alt_bwamem_GRCh38DH.20150706.CEU.illumina_platinum_ped.cram 4aa7f5b61d4365a556c980278b9be5a1 ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/illumina_platinum_pedigree/data/CEU/NA12893/alignment/NA12893.alt_bwamem_GRCh38DH.20150706.CEU.illumina_platinum_ped.cram.crai ae3d1ac0de67d58d192a96508bad85b4 ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/illumina_platinum_pedigree/data/CEU/NA12893/alignment/NA12893.alt_bwamem_GRCh38DH.20150706.CEU.illumina_platinum_ped.bam.bas 7fb61b29c6b5fc716e9167affab56d92&gt; ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/illumina_platinum_pedigree/data/CEU/NA12892/alignment/NA12892.alt_bwamem_GRCh38DH.20150706.CEU.illumina_platinum_ped.cram 8acdcd17349546b5ca1b45111e30fc07 ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/illumina_platinum_pedigree/data/CEU/NA12892/alignment/NA12892.alt_bwamem_GRCh38DH.20150706.CEU.illumina_platinum_ped.cram.crai 555de96d6baf2e16630dadd9a6b5b038 ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/illumina_platinum_pedigree/data/CEU/NA12892/alignment/NA12892.alt_bwamem_GRCh38DH.20150706.CEU.illumina_platinum_ped.bam.bas 5079d6d9dfcb0eb4631d11035ec71b16&gt; &gt; Further informationFor further information, please contact info@1000genomes.org. README_ftp_site_structure.md—ftp网站文件结构 Structure of the FTP siteThis file provides an overview of the structure of the FTP site. Top level of the siteAt the top level of the site, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/, there are a number of files and directories. Files present in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/: README files: These provide information on a variety of topics related to this FTP site and the data it contains. CHANGELOG file: This file records alterations made to this FTP site. current.tree file: This file lists all directories and files currently present on this FTP site. Directories present in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/: changelog_details data data_collections historical_data phase1 phase3 pilot_data release technical These directories and described in the next section of this file. Directorieschangelog_detailsThis directory contains a series of files detailing the changes made to the FTP site over time. dataThe data directory formerly housed data generated during the 1000 Genomes Project. The data that was previously located here has been integrated into data_collections and is present under ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data. Further information on this move can be found in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/README_data_has_moved.md. data_collectionsThe data_collections directory contains directories for various collections of data, typically generated by different projects. Among the data collections is the 1000 Genomes Project data. For each collection of data, within the directory for that collection, README and index files provide information on the collection. Under each collection directory, there is a data directory, under which files are organised by population and then sample. Further information can be found in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/README_data_collections.md. historical_dataThis directory was created during a rearrangement of the FTP site in September 2015. It houses README and index files that were formerly present at the toplevel of this site, including dedicated index directories. Further information is available in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/historical_data/README_historical_data.md. phase1This directory contains data that supports the publications associated with phase 1 of the 1000 Genomes Project. phase3This directory contains data that supports the publications associated with phase 3 of the 1000 Genomes Project. pilot_dataThis directory contains data that supports the publications associated with the pilot phase of the 1000 Genomes Project. releaseThe release directory contains dated directories which contain analysis results sets plus README files explaining how those data sets were produced. Originally, the date in release subdirectory names was the date on which the given release was made. Thereafter, the release subdirectory dates were based on the date in the name of the corresponding YYYYMMDD.sequence.index file. In future, the date in the directory name will be chosen in a manner appropriate to the data and the nature of the release. Examples of release subdirectories are: ftp://ftp.1000genomes.ebi.ac.uk/release/2008_12/ ftp://ftp-trace.ncbi.nih.gov/1000genomes/release/2008_12/ In cases where release directories are named based on the date of the YYYYMMDD.sequence.index, the SNP calls, indel calls, etc. in these directories are based on alignments produced from data listed in the YYYYMMDD.sequence.index file. For example, the directoryftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20100804/contains the release versions of SNP and indel calls based on theftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/historical_data/former_toplevel/sequence_indices/20100804.sequence.indexfile. technicalThe technical directory contains subdirectories for other data sets such as simulations, files formethod development, interm data sets, reference genomes, etc.. An example of data stored under technical is ftp://ftp.1000genomes.ebi.ac.uk/technical/simulations/. WARNING: ftp://ftp.1000genomes.ebi.ac.uk/technical/working/ The working directory under technical contains data that has experimental (non-public release) statusand is suitable for internal project use only. Please use with caution. Further informationShould you require further assistance in navigating the FTP site, please contact info@1000genomes.org. README_missing_files.md—缺失文件 The 1000 Genomes FTP site under goes periodic rearrangments to accomodate new data sets coming inand to preserve old data in a new location. If there is a particular file where you don’t know if it still exists, the best place to try isour current.tree file. ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/current.tree This file contains five columns: relative path type (directory or file) size in bytes last updated time stamp md5 You should be able to search this file with your filename and see if the file still exists and find its new location. If you still can’t find your file email info@1000genomes.org and the support team will be able to tell you what hashappened to it. README_populations.md—人群 PopulationsThis file describes the population codes where assigned to samples collected for the 1000 Genomes project. These codes are used to organise the files in the data_collections’ project data directories and can also be found in column 11 of many sequence index files. There are also two tsv files, which contain the population codes and descriptions for both the sub and super populations that were used in phase 3 of the 1000 Genomes Project: ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/20131219.populations.tsvftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/20131219.superpopulations.tsv Populations and codes CHB Han Chinese Han Chinese in Beijing, China JPT Japanese Japanese in Tokyo, Japan CHS Southern Han Chinese Han Chinese South CDX Dai Chinese Chinese Dai in Xishuangbanna, China KHV Kinh Vietnamese Kinh in Ho Chi Minh City, Vietnam CHD Denver Chinese Chinese in Denver, Colorado (pilot 3 only) CEU CEPH Utah residents (CEPH) with Northern and Western European ancestry TSI Tuscan Toscani in Italia GBR British British in England and Scotland FIN Finnish Finnish in Finland IBS Spanish Iberian populations in Spain YRI Yoruba Yoruba in Ibadan, Nigeria LWK Luhya Luhya in Webuye, Kenya GWD Gambian Gambian in Western Division, The Gambia MSL Mende Mende in Sierra Leone ESN Esan Esan in Nigeria ASW African-American SW African Ancestry in Southwest US ACB African-Caribbean African Caribbean in Barbados MXL Mexican-American Mexican Ancestry in Los Angeles, California PUR Puerto Rican Puerto Rican in Puerto Rico CLM Colombian Colombian in Medellin, Colombia PEL Peruvian Peruvian in Lima, Peru GIH Gujarati Gujarati Indian in Houston, TX PJL Punjabi Punjabi in Lahore, Pakistan BEB Bengali Bengali in Bangladesh STU Sri Lankan Sri Lankan Tamil in the UK ITU Indian Indian Telugu in the UK Should you have any queries, please contact info@1000genomes.org. README_using_1000genomes_cram.md—cram文件读取 IGSR CRAM TutorialFrom the first release of GRCh38 alignments onwards, we are releasing our alignment files in CRAM format. CRAM is a reference-based compression of sequence data. Both htslib and picard can read CRAM files, many standard tools should be able to read these files natively. Here are details about how to view CRAM files, convert from CRAM to BAM, how we produced the cram files and the CRAM specification. Using CRAM filesCRAM Files can be read by both samtools and picard. EMBL-EBI also provides a java API called cramtools (http://www.ebi.ac.uk/ena/software/cram-toolkit) Reading a CRAM file with samtools - samtools view commands work with CRAM files. This functionality needs samtools v1.2 or higher &gt;samtools view $input.cram -h chr22:1000000-1500000 | less Converting a CRAM file to a BAM File - some tools still need BAM files rather than CRAM. You can convert from CRAM to BAM easily &gt;java -jar cramtools-3.0.jar bam -I $input.cram -R $reference.fa -O $output.bam Please note the first time you run these commands the program reading the CRAM file must download the reference sequence data from an online cache. This process can be speeded up if you download the required reference file and build a local copy of the cache in advance. This process is described below in the CRAM reference registry section. The CRAM reference registryBecause CRAM does not contain the same level of sequence data as BAM files, it relies on the CRAM reference registry to provide reference sequences for CRAM to output uncompressed sequences. The reference must be available at all times. Losing it is equivalent to losing all your read sequences. Retrieval of reference data from the registry is supported by using MD5 or SHA1 checksums using the following URLs: www.ebi.ac.uk/ena/cram/md5/&lt;hashvalue&gt;www.ebi.ac.uk/ena/cram/sha1/&lt;hashvalue&gt; The md5 values for all GRCh38 reference chromosomes and contigs are included in CRAM file headers. These are mandatory fields in the CRAM specification. The following process can be used to download and prepare the cache in advance to speed up initial reads of the sequence data from CRAM files. Download the reference file from our FTP site. ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa Run the seq_cache_populate.pl script from samtools - http://www.htslib.org/workflow/#mapping_to_cram&gt;perl samtools/misc/seq_cache_populate.pl -root /path/to/cache /path/to/GRCh38_full_analysis_set_plus_decoy_hla.fa Set the cache environment variables&gt;export REF_PATH=/path/to/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s&gt;export REF_CACHE=/path/to/cache/%2s/%2s/%s How did we compress the BAMs to CRAMsWe use cramtools to convert the BAM files our alignment pipeline produces to CRAM Files. The files are compressed using a lossy mode, this bins all quality stores into the 8-binning scheme defined by Illumina. &gt;java -jar cramtools-3.0.jar cram --ignore-tags OQ:CQ:BQ --capture-all-tags --lossy-quality-score-spec &#39;*8&#39; --preserve-read-names -O $output.cram -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I $input.bam More information about CRAM format.As mentioned above CRAM represents a reference-based compression of sequence data. After aligning sequences reads to a reference genome, rather than storing every base pair of a sequence read, the approach stores only the difference between the read and the reference, hence reducing the space needed for storing sequence reads. Additional compression can be archived in lossy mode by controlled loss of quality information and unaligned reads, by dropping read names and other information. The level of compression can be fine tuned based on users’ experiment design. With associated tools, the compressed data can be seamlessly uncompressed and fed into downstream analysis. This compression method was first developed by Ewan Birney’s group in European Bioinformatics Institute (EBI) (Hsi-Yang Fritz, et al. (2011). Genome Res. 21:734-740). The specification itself is maintained by the HTSlib group alongside the BAM, VCF and BCF specifications http://samtools.github.io/hts-specs/CRAMv3.pdf. If you have any questions about our CRAM files or our alignment pipeline, please email info@1000genomes.org 1000 Genomes 数据库结构-data_collections该文件主要存储基因组的2000人的数据，本层数据说明文件如下： Data collectionsCollectionsThe International Genome Sample Resource (IGSR) provides access to data from multiple projects, including the 1000 Genomes Project. As a consequence of this, data is organised in collections to reflect these different projects. Each of the directories in this directory contains data for a given collection. Data from the 1000 Genomes projects can be found under 1000_genomes_project. Collection layoutWithin each collection directory, you will find information in README files, which describe the data and any processing which has been done. In addition, index files provide a catalogue of the files available for that collection. The body of each index file is tab delimited. Index files also have a header section with lines starting with ##, providing information about the file, data and the columns. The column header is a line starting with a single # immediately before the body of the file. The sequence indices contain the locations of the sequence data in the European Nucleotide Archive (ENA). The data directory in each collection directory is where the data is housed. Data is organised by population and then by sample, for example, 1000_genomes_project/data/YRI/NA19150/. Further informationIf you are looking for a specific file, a list of all files on the site can be found in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/current.tree. Should you have any queries, please contact info@1000genomes.org. 1000G_2504_high_coverage这是1000基因组2504基因高覆盖数据集合，如下是他的说明文件 1000 Genomes 2504 phase 3 panel sequenced to high coverageThis README refers to 30x Illumina NovaSeq sequencing of 2504 samples from the 1000 Genomes project phase 3 sample set. These data were generated at the New York Genome Center with funds provided by NHGRI Grant 3UM1HG008901-03S1. Please email service@nygenome.org with questions or interest in undertaking collaborative analysis of this dataset. All cell lines were obtained from the Coriell Institute for Medical Research and were consented for full public release of genomic data. Please see Coriell (https://www.coriell.org) for more information about specific cell lines. The following cell lines/DNA samples were obtained from the NIGMS Human Genetic Cell Repository at the Coriell Institute for Medical Research: [NA06984, NA06985, NA06986, NA06989, NA06994, NA07000, NA07037, NA07048, NA07051, NA07056, NA07347, NA07357, NA10847, NA10851, NA11829, NA11830, NA11831, NA11832, NA11840, NA11843, NA11881, NA11892, NA11893, NA11894, NA11918, NA11919, NA11920, NA11930. NA11931, NA11932, NA11933,NA11992, NA11994, NA11995, NA12003, NA12004, NA12005, NA12006, NA12043, NA12044, NA12045, NA12046, NA12058, NA12144, NA12154, NA12155, NA12156, NA12234, NA12249, NA12272, NA12273, NA12275, NA12282, NA12283, NA12286, NA12287, NA12340, NA12341, NA12342, NA12347, NA12348, NA12383, NA12399, NA12400, NA12413,, NA12414, NA12489, NA12546, NA12716, NA12717, NA12718, NA12748, NA12749, NA12750, NA12751, NA12760, NA12761, NA12762, NA12763, NA12775, NA12776, NA12777, NA12778, NA12812, NA12813, NA12814, NA12815, NA12827, NA12828, NA12829, NA12830, NA12842, NA12843, NA12872, NA12873, NA12874, NA12878, NA12889, NA12890]. The sequence data is available in ENA and a listing of files, with metadata, is available in the accompanyiing index. AnalysisAnalysis work is being done by a number of groups, working toward variant calling, including identification of structural variation. Initial analysis has been done by NYGC, including aligning the data to GRCh38, creating the CRAMs in ENA. The document NYGC_b38_pipeline_description.pdf contains a description of that analysis work and details of the alignment pipeline. Should you have questions about this data please contact info@1000genomes.org 1000_genomes_project这是1000基因组的原始数据存储文件This directory contains sequence data generated by the 1000 Genomes Project, and alignment data of the reads to GRCh38.Subsequent analysis results such as variant call sets will be in this directory when they become available. Sequence DataMoving forward we are aligning the raw sequence data unfiltered, as such we no long rehost the fastq files but instead our sequence index files point you to the FTP url for the fastq files on ENA servers The data in the sequence index are categorised into 3 analysis groups (column 26) based on the library strategy used: low coverage - Low coverage whole genome sequencingexome - Whole exome sequencinghigh coverage - PCR-free high coverage whole genome sequencing When align to GRCh38, data from the three analysis groups were aligned independently, producing three sets of CRAM files. Some of the runs have a withdrawn flag (column 21); these runs were not included in subsequent analysis.Column 22 indicates the reason why a run is withdrawn. Below are the possible reasons: SUPPRESSED IN ARCHIVE - runs were withdrawn by submitters so the fastq file is no longer available from ENA/SRATOO_SHORT - alll reads in a whole genome sequencing run are shorter than 70bp or in a exome run shorter than 68bpNOT_ILLUMINA - a run generated on a platform that is not Illumina. For data consistency, we only include data produced on the Illumina platform for subsequent analysis. Alignment DataThe sequence data described by the sequence index file (1000genomes.sequence.index) were aligned to GRCh38 using ALT-aware bwa-mem. Please see details of the alignment process in README.1000genomes.GRCh38DH.alignment. Sample level CRAMs were produced; the CRAM files and ancillary files are listed in alignment index files in this directory. 总结1000基因组的数据文件非常大，存储达到1000G左右，所以存储是一个不小的问题，需要花费很多计算资源和数据存储资源，并且算法上面也是一个很高维度的计算方法才能对基因的问题有一个比较好的描述和解释，目前所采用的方法大多是传统的方法，就是因为目前的方法都是基于简单计算的，有很好的公式进行表示计算，但是人工智能领域的方法并没有很好的描述这个问题，现在的技术方法就是需要用人工智能的方法拟合传统公式所需要表达的思想，然后通过神经网络拟合的网络进行计算，得到更加精确的解释，但是这样也有一定的风险，因为人工智能的方法能不能刻画这个模型，或者刻画这个模型以后是不是有更好的精确解释，这是一个比较难表示的问题。]]></content>
      <categories>
        <category>Gene</category>
      </categories>
      <tags>
        <tag>Gene常识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下搭建多用户多权限FTP]]></title>
    <url>%2F2019%2F07%2F07%2FUbuntu%E4%B8%8B%E6%90%AD%E5%BB%BA%E5%A4%9A%E7%94%A8%E6%88%B7%E5%A4%9A%E6%9D%83%E9%99%90FTP%2F</url>
    <content type="text"><![CDATA[Descriptionvsftp—VSFTP是一个基于GPL发布的类Unix系统上使用的FTP服务器软件。关于这个软件的详细信息大家可以自行百度。 切换root用户模式 (已在root模式下的请忽略)1sudo -i 然后提示你输入当前用户密码，输入密码后回车后如果密码正确控制台就变成root@***:/#了。 更新软件源且安装vsftp12sudo apt-get updateapt-get install vsftpd 创建工作目录为了操作方便管理，我们给FTP创建自定义FTP目录和用户组,创建FTP目录 /home/ftp 当然你也可以创建到其它目录1mkdir /home/ftp 创建FTP用户组 ftp-g 这个你们也可以用其它组名1groupadd ftp-g 设置FTP目录读写权限(注：用户登陆的根目录如果权限设置的太大会导致无法登陆，根目录下的子目录可以给很大的权限，一般服务器都是采用755权限1chmod 755 /home/ftp 设置文件夹归属 root 以及 我们新建的 FTP用户组1chown root:ftp-g /home/ftp 到此，ftp的根目录就已经创建完成了 设置FTP用户目录以及添加FTP用户为用户ftp_pub 创建目录 pub并设置权限12mkdir /home/ftp/pubchmod 755 /home/ftp/pub 添加用户 ftp_pub并设置归属用户组为 ftp-g 以及设置缺省目录(FTP登陆目录) -s /sbin/nologin 禁止用户登陆控制台1useradd -g ftp-g -d /home/ftp/pub -s /sbin/nologin ftp_pub 给新用户ftp_pub设置密码(无密码不能登陆，然后按照提示输入两遍密码，比如我们设置为 1234567981passwd ftp_pub 更改目录归属,ftp下新建的这个目录只有ftp_pub才能够被更改1chown ftp_pub:ftp-g /home/ftp/pub 增加上传目录不可删除功能，在配置文件中增加user_config_dir=/etc/vsftpd_user_ctrl，同时在vsftpd_user_ctrl文件夹中增加对应的账户名字的文件，目的就是对指定的账户进行操作限制 增加的指令,cmds_denied=DELE，就是防止删除操作，同样的指令操作还有很多，可以到网络上面自行百度。到此，就实现了普通用户的操作限制，比如限制指定用户删除操作 给FTP开放外网端口(默认:tcp端口 21)1ufw allow 21/tcp 启动FTP服务1/etc/init.d/vsftpd start 总结普通用户还是比较好搭建服务器，关键是管理员权限不好控制，看来是需要更加复杂的控制才行，需要通过单独的文件进行控制，比如用vsftp的用户控制或者用用户的权限控制，比如登陆以后给与某个用户超级，最后还是解决了超级管理员的问题，在星期四晚上的最后一刻，还是解决了超级管理员的问题，就是设置root可以登录，同时登陆root账户，那么所有的文件权限也都有了 下面是两个比较成熟的vsftp搭建教程：https://blog.51cto.com/meiling/2071122 安装完以后会出现当前用户无法访问当前目录的问题，下面的教程会解决这个问题，会提示1500 OOPS: vsftpd: refusing to run with writable root inside chroot () 原因是： 从2.3.5之后，vsftpd增强了安全检查，如果用户被限定在了其主目录下，则该用户的主目录不能再具有写权限了！如果检查发现还有写权限，就会报该错误。 要修复这个错误，可以用命令chmod a-w /home/user去除用户主目录的写权限，注意把目录替换成你自己的。或者你可以在vsftpd的配置文件中增加下列两项中的一项：123456#方案一$ chmod a-w /vaf/ftp #方案二$ vim /etc/vsftpd.conf add the following allow_writeable_chroot=YES 解决上述问题后，可能还会出现ftp客服端乱码等问题，可以通过设置客户端强制为utf-8字符格式，然后打开vsftpd.conf里面的utf-8模式，客户端检测到utf-8模式以后就会自动采用该模式上传文件，可以解决中文乱码的问题。 注意（FTP上传下载速度问题）考虑过我们ftp的实际速度的问题吗，为何我们不能达到100M以上的速度？ 先讲一点网络速率和传输速度的问题呵呵，我先是一个CCNP，然后才是一个DBA 注意下面B－字节 b－位的区别 1B=8b,不理解的回去看计算机基础知识 1000M网络速度是指bit位的速度，理论网络传输速度上限是1000/8=128MB/S 可是这个速度是在物理层的理论值,呵呵我们要用的TCP/IP协议和FTP协议中间还有好多个协议层.基于种种原因的考虑,每个协议层都要在数据包头封装一些东西,因此我们一般只能实现用10个字节来描述一个有效的数据 换算成传输速率就是1000/10=100MB/S 另外要考虑的一个事情是,你的应用是否是单向传输的业务,如果是的话,你最多只能实现100MB/S的速率,如果是双向对称传输的协议话呵呵,你的最大传输速率是100/2=50MB/S 这样的业务典型的就是双向的视频会议. 回到FTP传输的问题来,大家要问,FTP不是单方向传输的吗?为何我们经常只能实现30-40MB/S,最多不过60-70MB/S的速度,不对FTP是一个典型的不对称的传输协议,在我们下载的时候,可能下载流量占用了大部分的带宽,但是大家要注意到,FTP必须在服务器和客户端之间传输一些控制命令和交换一些数据请求包什么的,典型的上行流量大概能占用到整个传输10%,呵呵我们上面提到的100M/S的理论上限一下少了10M/S变成90M/S. 接着你又问了不还有90M/S吗?剩下那些速度都在那里产生瓶颈了,呵呵,让我慢慢道来： 你的客户端和服务器最少都可能在同一台千兆交换机上吧,对不起,交换机会对数据传输产生一定的时延,因为千兆交换机一定是基于存储转发的,这个时延根据厂商算法和芯片的不同,可以造成传输速率下降,下降的幅度在2-10%之间变动,按照平均5%来计算吧,现在是多少了,90*0.95=85.5MB/S 服务器的千兆网卡和客户机的千兆网卡是接在那里的,你一定会说是在主板上集成的,没错,可是你知道它们到底接在那里吗?呵呵告诉你如果是接在主板上的一定是在南桥总线上的,典型的南桥总线是2GB/S(INTEL ICH8),没完这南桥总线不是千兆网卡独享的,是N多设备共享的,网卡,IDE硬盘接口/SATA接口,声卡等等一大堆东西,一般来说,网卡能达到理论带宽的90%就很不错了,呵呵再算一下,85.5*0.9=76.95MB/S 没完,FTP协议使用网络传输的数据从那里来?呵呵从硬盘来,到那里去?到硬盘去.这一来一去有个问题,硬盘并非能全速达到理论传输上限,哪怕你组成一个很快的RAID 0,硬盘传输数据都会有一点延迟,这个延迟来自于硬盘的传输原理,FTP一个大文件要在磁盘中移动磁头,找到这个数据块,然后读到内存中,有个5-10%的延迟不为过吧,写入同样,要找到空闲的块,同样可能有5-10%的延迟,取小一点,两边都按5%来计算,呵呵10%的延迟呀,现在你一定冒汗了,降到多少了?76.95*0.9=69.255MB/S 数据仅仅是到了内存,还没传输到网卡上,这时后,CPU肯定要处理,DMA通道也要处理,内存虽然读得飞快,网卡处理芯片要处理数据,这一大堆玩意虽然都很快,可是同样有延迟,考虑到这些因素,降个5%是很正常的,实际上,基于内存的网络传输速率测试不比硬盘快多少,这时的速率是69.255*0.95=65.79MB/S这个速率我想大多数使用过千兆网络FTP传输的人,都见过这样的速度吧,一般来说千兆网络FTP传输很难超过80M/S 操作系统,FTP SERVER 和FTP CLINET的软件处理能力的问题,这个是一个更复杂的问题,但是可以肯定的是不同的平台和版本,性能差异可能巨大,我用过N多的FTP SERVER和FTP CLINET,访问同一个内部服务器的时候,性能差异巨大,相同的环境下,例如IIS 6的SERVER 配合WINDOWS FTP命令能到40-50MB/S,换一个LINUX下的LFTP客户端就能达到60-65MB/S的性能. 这上面还有一个因素,如果遇到交换机繁忙,或者交换机交换容量不够的情况下,上面许多因素再打大一点折扣,例如磁盘延迟很大,上面的数字还要大大打个折扣.不幸的是我们的许多系统大部分的部件,经常处于繁忙的阶段,上面的数字最多只能实现80%的效能,呵呵所以大多数的千兆FTP传输在30-40MB/S间波动是很正常的.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>FTP搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[历年顶会自动摘要论文合集]]></title>
    <url>%2F2019%2F07%2F04%2F%E5%8E%86%E5%B9%B4%E9%A1%B6%E4%BC%9A%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[Description文本摘要是自然语言处理中比较难的一个任务，别说是用机器来做文摘了，就连人类做文摘的时候都需要具备很强的语言阅读理解能力和归纳总结能力。新闻的摘要要求编辑能够从新闻事件中提取出最关键的信息点，重新组织语言来写摘要；paper的摘要需要作者从全文中提取出最核心的工作，然后用更加精炼的语言写成摘要；综述性的paper需要作者通读N篇相关topic的paper之后，用最概括的语言将每篇文章的贡献、创新点写出来，并且对比每篇文章的方法各有什么优缺点。自动文摘本质上做的一件事情是信息过滤，从某种意义上来说，和推荐系统的功能有一点像，都是为了让大家更快地找到感兴趣的东西，只是用了不同的手段而已。 2013年顶会论文Mikolov, T., et al. (2013) Efficient Estimation of Word Representations in Vector Space. ArXiv e-prints 2014年顶会论文Bahdanau, D., et al. (2014). “Neural machine translation by jointly learning to align and translate.” arXiv preprint arXiv:1409.0473. 2015顶会论文Rush, A. M., et al. (2015). A Neural Attention Model for Abstractive Sentence Summarization, Association for Computational Linguistics. 2016顶会论文 Cao, Z., et al. (2016). AttSum: Joint Learning of Focusing and Summarization with Neural Attention, The COLING 2016 Organizing Committee. Cheng, J. and M. Lapata (2016). Neural Summarization by Extracting Sentences and Words, Association for Computational Linguistics. Chopra, S., et al. (2016). Abstractive Sentence Summarization with Attentive Recurrent Neural Networks, Association for Computational Linguistics. Hsieh, Y.-L., et al. (2016). 運用序列到序列生成架構於重寫式自動摘要(ExploitingSequence-to-Sequence Generation Framework for Automatic Abstractive Summarization)[In Chinese], The Association for Computational Linguistics and Chinese Language Processing (ACLCLP). Iyer, S., et al. (2016). Summarizing Source Code using a Neural Attention Model, Association for Computational Linguistics. Jadon, M. K. and A. Pareek (2016). A method for Automatic Text Summarization using Consensus of Multiple Similarity Measures and Ranking Techniques, NLP Association of India. Kim, M., et al. (2016). Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization, Association for Computational Linguistics. Kim, Y., et al. (2016). Character-aware neural language models. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. Phoenix, Arizona, AAAI Press: 2741-2749. Li, C., et al. (2016). Using Relevant Public Posts to Enhance News Article Summarization, The COLING 2016 Organizing Committee. Li, J. J., et al. (2016). The Role of Discourse Units in Near-Extractive Summarization, Association for Computational Linguistics. Li, W., et al. (2016). Abstractive News Summarization based on Event Semantic Link Network, The COLING 2016 Organizing Committee. Luo, W., et al. (2016). An Improved Phrase-based Approach to Annotating and Summarizing Student Course Responses, The COLING 2016 Organizing Committee. Mehta, P. (2016). From Extractive to Abstractive Summarization: A Journey, Association for Computational Linguistics. Nallapati, R., et al. (2016). Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond, Association for Computational Linguistics. Polsley, S., et al. (2016). CaseSummarizer: A System for Automated Summarization of Legal Texts, The COLING 2016 Organizing Committee. Zopf, M., et al. (2016). Beyond Centrality and Structural Features: Learning Information Importance for Text Summarization, Association for Computational Linguistics. 2017顶会论文 Bossard, A. and C. Rodrigues (2017). An Evolutionary Algorithm for AutomaticSummarization, INCOMA Ltd. Chali, Y., et al. (2017). Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging, Asian Federation of Natural Language Processing. Hua, X. and L. Wang (2017). A Pilot Study of Domain Adaptation Effect for Neural Abstractive Summarization, Association for Computational Linguistics. Isonuma, M., et al. (2017). Extractive Summarization Using Multi-Task Learning with Document Classification, Association for Computational Linguistics. J Kurisinkel, L., et al. (2017). Abstractive Multi-document Summarization by Partial Tree Extraction, Recombination and Linearization, Asian Federation of Natural Language Processing. Lee, G. H. and K. J. Lee (2017). Automatic Text Summarization Using Reinforcement Learning with Embedding Features, Asian Federation of Natural Language Processing. Li, P., et al. (2017). Deep Recurrent Generative Decoder for Abstractive Text Summarization. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics. Miller, J. and K. McCoy (2017). Topic Model Stability for Hierarchical Summarization, Association for Computational Linguistics. Nema, P., et al. (2017). Diversity driven attention model for query-based abstractive summarization, Association for Computational Linguistics. Neubig, G. (2017) Neural Machine Translation and Sequence-to-sequence Models: A Tutorial. ArXiv e-prints Ouyang, J., et al. (2017). Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora, Association for Computational Linguistics. Rücklé, A. and I. Gurevych (2017). Real-Time News Summarization with Adaptation to Media Attention, INCOMA Ltd. Schluter, N. (2017). The limits of automatic summarisation according to ROUGE, Association for Computational Linguistics. See, A., et al. (2017). Get To The Point: Summarization with Pointer-Generator Networks, Association for Computational Linguistics. Suzuki, J. and M. Nagata (2017). Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization, Association for Computational Linguistics. Tan, J., et al. (2017). Abstractive Document Summarization with a Graph-Based Attentional Neural Model, Association for Computational Linguistics. Xu, Y., et al. (2017). Decoupling Encoder and Decoder Networks for Abstractive Document Summarization, Association for Computational Linguistics. Yang, Y., et al. (2017). Detecting (Un)Important Content for Single-Document News Summarization, Association for Computational Linguistics. Yasunaga, M., et al. (2017). Graph-based Neural Multi-Document Summarization, Association for Computational Linguistics. Zhou, Q., et al. (2017). Selective Encoding for Abstractive Sentence Summarization, Association for Computational Linguistics. 2018年顶会论文 Amplayo, R. K., et al. (2018). Entity Commonsense Representation for Neural Abstractive Summarization, Association for Computational Linguistics. Cao, Z., et al. (2018). Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization, Association for Computational Linguistics. Chen, J. and H. Zhuge (2018). Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN, Association for Computational Linguistics. Chen, Y.-C. and M. Bansal (2018). Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting, Association for Computational Linguistics. Cohan, A., et al. (2018). A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents, Association for Computational Linguistics. Devlin, J., et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv e-prints Fan, A., et al. (2018). Controllable Abstractive Summarization, Association for Computational Linguistics. Gehrmann, S., et al. (2018). Bottom-Up Abstractive Summarization, Association for Computational Linguistics. Grusky, M., et al. (2018). Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies, Association for Computational Linguistics. Hardy, H. and A. Vlachos (2018). Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation, Association for Computational Linguistics. Hsu, W.-T., et al. (2018). A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss, Association for Computational Linguistics. Jadhav, A. and V. Rajan (2018). Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks, Association for Computational Linguistics. Jain, P., et al. (2018). A Mixed Hierarchical Attention Based Encoder-Decoder Approach for Standard Table Summarization, Association for Computational Linguistics. Jiang, Y. and M. Bansal (2018). Closed-Book Training to Improve Summarization Encoder Memory, Association for Computational Linguistics. Kedzie, C., et al. (2018). Content Selection in Deep Learning Models of Summarization, Association for Computational Linguistics. Krishna, K. and B. V. Srinivasan (2018). Generating Topic-Oriented Summaries Using Neural Attention, Association for Computational Linguistics. Kryściński, W., et al. (2018). Improving Abstraction in Text Summarization, Association for Computational Linguistics. Lebanoff, L., et al. (2018). Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization, Association for Computational Linguistics. Li, W., et al. (2018). Improving Neural Abstractive Document Summarization with Structural Regularization, Association for Computational Linguistics. Lin, J., et al. (2018). Global Encoding for Abstractive Summarization, Association for Computational Linguistics. Liu, Y., et al. (2018). Controlling Length in Abstractive Summarization Using a Convolutional Neural Network, Association for Computational Linguistics. Mathur, P., et al. (2018). “Multi-lingual neural title generation for e-Commerce browse pages.” arXiv preprint arXiv:1804.01041. Mitcheltree, C., et al. (2018). Using Aspect Extraction Approaches to Generate Review Summaries and User Profiles, Association for Computational Linguistics. Narayan, S., et al. (2018). Ranking Sentences for Extractive Summarization with Reinforcement Learning, Association for Computational Linguistics. ShafieiBavani, E., et al. (2018). Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings, Association for Computational Linguistics. Shang, G., et al. (2018). Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization, Association for Computational Linguistics. Song, K., et al. (2018). Structure-Infused Copy Mechanisms for Abstractive Summarization, Association for Computational Linguistics. Sulem, E., et al. (2018). Semantic Structural Evaluation for Text Simplification, Association for Computational Linguistics. Sulem, E., et al. (2018). Simple and Effective Text Simplification Using Semantic and Neural Methods, Association for Computational Linguistics. Wang, Y. and H.-y. Lee (2018). Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks, Association for Computational Linguistics. Wang, Y., et al. (2018). Neural Related Work Summarization with a Joint Context-driven Attention Mechanism, Association for Computational Linguistics. Zhou, Q., et al. (2018). Neural Document Summarization by Jointly Learning to Score and Select Sentences, Association for Computational Linguistics.]]></content>
      <categories>
        <category>论文</category>
        <category>论文合集汇总</category>
      </categories>
      <tags>
        <tag>自动摘要汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BERT的前世今生]]></title>
    <url>%2F2019%2F07%2F03%2FBERT%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[DescriptionBERT是一种预训练语言表示的方法，在大量文本语料（维基百科）上训练了一个通用的“语言理解”模型，然后用这个模型去执行想做的NLP任务。BERT比之前的方法表现更出色，因为它是第一个用在预训练NLP上的无监督的、深度双向系统。无监督意味着BERT只需要用纯文本语料来训练，这点非常重要，因为海量的文本语料可以在各种语言的网络的公开得到。 TransformerTransformer来自论文: All Attention Is You Need 别人的总结资源： 谷歌官方AI博客: Transformer: A Novel Neural Network Architecture for Language Understanding Attention机制详解（二）——Self-Attention与Transformer谷歌软件工程师 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较中科院软件所 · 自然语言处理 /搜索 10年工作经验的博士（阿里，微博）； Calvo的博客：Dissecting BERT Part 1: The Encoder，尽管说是解析Bert，但是因为Bert的Encoder就是Transformer，所以其实它是在解析Transformer，里面举的例子很好； 再然后可以进阶一下，参考哈佛大学NLP研究组写的“The Annotated Transformer. ”，代码原理双管齐下，讲得也很清楚。 《Attention is All You Need》浅读（简介+代码）这个总结的角度也很棒。 A High-Level Look可以将输入的语言序列转换成另外一种序列，比如下图的神经机器翻译： Transformer模型由编码器-解码器组合组成，解码器负责对序列进行编码，提取时间和空间信息，解码器负责利用时间和空间特征信息进行上下文预测，下图是单个结构： 编码器和解码器堆栈的组合结构，在谷歌的实验结构中采用了6个编码器和6解码器相对应，使模型的编码能力和解码能力达到一个平衡状态（堆栈式结构）： 编码器-解码器的内部结构，类似seq2seq模型： seq2seq模型： Encoder: 由6个相同的层组成, 每层包含两个sub-layers.第一个sub-layer就是multi-head attention layer，然后是一个简单的全连接层。其中每个sub-layer都加了residual connection（残差连接）和normalisation（归一化）。 Decoder: 由6个相同的层组成，这里的layer包含三个sub-layers, 第一个sub-layer 是masked multi-head attention layer。这里有个特别点就是masked, 作用就是防止在训练的时候，使用未来的输出的单词。比如训练时，第一个单词是不能参考第二个单词的生成结果的。Masked是在点乘attention操作中加了一个mask的操作，这个操作是保证softmax操作之后不会将非法的values连到attention中，提高泛化性。 解码器的Attention： decoder第一级自注意力的key, query, value均来自前一层decoder的输出，但加入了Mask操作，即我们只能attend到前面已经翻译过的输出的词语，因为翻译过程我们当前还并不知道下一个输出词语，这是我们之后才会推测到的。 decoder第二级注意力也被称作encoder-decoder attention layer，即它的query来自于之前一级的decoder层的输出，但其key和value来自于encoder的输出，这使得decoder的每一个位置都可以attend到输入序列的每一个位置。 总结一下，k和v的来源总是相同的，q在encoder及decoder自注意力层中与k,v来源相同，在encoder-decoder attention layer中与k,v来源不同。 编码器与解码器的连接： 编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适。 Self-Attention at a High Level假设下面的句子就是我们需要翻译的输入句： ”The animal didn’t cross the street because it was too tired” 当模型处理单词的时候，self attention层可以通过当前单词去查看其输入序列中的其他单词，以此来寻找编码这个单词更好的线索。 Self-Attention in Detail第一步是将输入的嵌入词向量通过三个不同的参数矩阵得到三个向量，分别是一个Query向量，一个Key向量和一个Value向量，参数矩阵分别为Wq，Wk，Wv，，如下图所示： 第二步是通过当前词的q向量与其他词的k向量计算当前词相对于其他词的得分，分数采用点积进行计算，如下图所示： 第三步和第四步是讲得到的分数除以k值维数的平方根（k值维数为64，可以使训练过程有更加稳定的梯度，这个归一化的值是经验所得），再通过softmax得到每个得分的标准化得分： 第五步是对当前词所得到的标准化值对所有value向量进行加权求和得到当前词的attention向量，这样就使不同单词的嵌入向量有了attention的参与，从而预测上下文句子的时候体现不同的重要的重要程度。 Matrix Calculation of Self-Attention Attendtion向量计算的矩阵形式，通过全职矩阵进行词向量的计算大大加快了神经网络的速度 X矩阵中的每一行对应于输入句子中的一个单词。（图中的4个方框论文中为512个）和q / k / v向量（图中的3个方框论文中为64个） 公式中浓缩前面步骤2到5来计算self attention层的输出。 The Beast With Many Heads使用“Multi-headed”的机制来进一步完善self-attention层。“Multi-headed”主要通过两个方面改善了Attention层的性能，参数组成和子空间映射： Many Heads的优缺点： 它拓展了模型关注不同位置的能力。Multi head 的每个参数矩阵都会记录单词的位置信息，使原来的单个位置信息变得更加复杂。 它为attention层提供了多个“representation subspaces”。由下图可以看到，在self attention中，我们有多个个Query / Key / Value权重矩阵（Transformer使用8个attention heads），使特征的提取变得更加复杂，而不是作为一个整体的特征进行，每个单独的子空间都会进行上下文的信息融合 在8个不同的子空间进行self-attention的操作，每个单词生成独立的8个向量 将8个子空间生成的向量压缩成一个大向量，每个向量的子空间矩阵能够学习到更多细节，压缩过程采用一个更大的参数矩阵进行，对multi-head向量进行组合，生成最终的特征向量。 整体的框图来表示一下计算的过程： Representing The Order of The Sequence Using Positional Encoding其实上面介绍的网络里面并没有考虑序列的位置信息，在RNN中不同时刻的信息是通过递归网络的时间t来刻画的，有明显的时间刻度，所以引入了位置向量来解决时间刻度问题。 为了让模型捕捉到单词的顺序信息，添加位置编码向量信息（POSITIONAL ENCODING），位置编码向量不需要训练，它有一个规则的产生方式，生成与词嵌入向量有着相同的向量就可以。 通过构造函数sin、cos来对位置进行嵌入，pos为单词位置信息，而i用来表达dimension 这里为了好说明，如果2i= dmodel, PE 的函数就是sin(pos/10000)。这样的sin, cos的函数是可以通过线性关系互相表达的，通过两个函数对奇偶维度进行编码。位置编码的公式如下图所示： 个人认为选择正余弦函数主要是在-1和1之间是一个对称关系，两个相邻的维度编码相差比较大，在位置上有更好的区分性，1000是序列的长度，一般尽量将取值范围控制在四分一个周期里面，这样会使每一个序列的每一个维度都取唯一的值。 The Residuals编码器和解码器里面的每一层都采用残差的思想进行训练，目的就是为了解决网络过深情况下的难训练问题，残差连接可以将目标值问题转化成零值问题，一定程度也可以减少网络的过拟合问题。 使用残差连接的编码器内部结构： 使用残差连接的编码器-解码器内部结构： The Decoder Side通过自回归方式进行预测，解码器每一个时间步输入一个单词，然后输出一个单词，将预测的单词作为下一时刻的输入进行单词的预测，直到预测结束。 The Final Linear and Softmax Layer 线性层是一个简单的全连接神经网络，模型一次生成一个输出，我们可以假设模型从该概率分布中选择具有最高概率的单词并丢弃其余的单词。 对于最终句子的生成有2个方法：一个是贪婪算法（greedy decoding），一个是波束搜索（beam search）。 线性变换：解码器最终会输出一个实数向量。解码器输出后的线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的（字典维度），被称作对数几率（logits）的向量里。不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量，其中每个单元格对应某一个单词的分数。 softmax层：Softmax 层便会把那些分数变成概率（都为正数、和为1）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。 Bidirectional Encoder Representation from Transformers（BERT）Word Embedding 线性模型，主要是对高维空间进行映射，其实是对one-hot向量的空间转换。 通过神经网络对输入的词进行映射，获取词向量，一般有cbow和skip-gram两种方法，此方法训练的词向量与上下文无关，并没有参考位置信息，只是对词的有无进行参考，采用的是负采样，预测的时候进行的是一个二分类器，模型认为只要在下文中找出正确的词就认为是完成了任务。 尚未解决一词多义等问题。比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。 Embedding from Language Models（ELMO） ElMO采用双向的LSTM做上下文相关的任务，从前到后和后到前分别做一遍LSTM的encoding操作，从而获得两个方向的token联系。 Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。 ELMO的本质思想是： 事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。 一样的，在具体进行下游任务的时候，采用神经网络参数微调的方法根据不同的词的上下文环境对词向量进行调整，从而得到同一词的不同向量表示。 缺点： LSTM的抽取能力远远落后于Transformer，主要是并行计算能力 拼接方式融合双向特征能力偏弱 Bidirectional Encoder Representation from Transformers InformationBERT的模型架构 Google在论文中提到了两个不同模型规模的BERT: BERT BASE –和OpenAI Transformer模型的规模差不多，方便与其进行性能比较 BERT LARGE – 一个达到目前多个benchmark的SOTA的巨大的模型 BERT基本上就是一个训练好的Transformer编码器栈。关于Transformer的内容可以看看 图解Transformer这篇博文。 两种规模的BERT模型都有许多编码器层 (在论文中称为“Transformer块”) – BERT Base有12个这样的结构，BERT Large有24个。编码器中也有前馈网络 (BERT Base中的是768个隐层神经元，BERT Large中的是1024个隐层神经元)， 以及注意力层中使用了比Transformer那篇论文中更多的“头” （BERT Base有12个“头”，BERT Large中有16个）。 BRET采用两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。在预训练阶段采用了类似ELMO的双向语言模型，双向指的是对于预测单词的上文和下文是否参与，如果都参与预测那么就是双向，双向容易导致自己看自己的问题，后面提出mask来解决 经过预训练的BRET模型，其已经具备了丰富的词向量特征信息，然后将此词向量信息与下游任务进行组合进行NLP下游任务，例如文本生成，文本分类。 如何能够更好将BRET模型与下游任务进行改造是一个比较复杂的问题，再好的预训练语言模型都要与下游的任务模型相结合才有好的效果， BRET的优势在于可以自由根据预训练模型进行单词级别的任务和句子级的任务。 BRET模型的输入输入序列的第一个token是一个特殊的符号[CLS]，这里的CLS代表class。 就像Transformer的编码器一样，BERT以一串单词作为输入，这些单词不断地想编码器栈上层流动。每一层都要经过自注意力层和前馈网络，然后在将其交给下一个编码器。 在体系结构方面，到目前为止，还是与Transformer是相同的（除了一些超参数之外）。接下来在输出端，我们会看到其和Transformer的不同之处。 BRET模型的输出每个位置对应地输出一个维度为hidden_size(BERT Base中为768)的向量。对于之前提到的句子分类的例子，我们只关注第一个位置的输出（也就是被我们用[CLS]符号代替的位置）。 输出的这个向量现在可以用作我们选择的分类器的输入。论文利用一个单层神经网络作为分类器，就能取得较好的分类效果。 如果你有更多的标签（例如，如果你是一个电子邮件服务提供商，你需要将电子邮件标记为“垃圾邮件”、“非垃圾邮件”、“社交”和“促销”等等），你只需调整分类器这部分的网络，使其具有更多的输出神经元，然后通过softmax。 与卷积网络并行对于有CV背景的人来说，这种向量传递应该让人想起像VGGNet这样的网络的卷积部分和网络结构最后的全连接层之间发生的事情。 BERT在下游任务中使用迁移学习既然OpenAI Transformer已经经过了预训练，而且它的各个层也经过了调整，我们就可以开始在下游任务中使用它了。让我们先来看看句子分类（将邮件信息分为“垃圾邮件”或“非垃圾邮件”）: OpenAI的论文列出了许多用于处理不同类型任务输入的输入变换。下图显示了模型的结构和执行不同任务时的输入变换。 BERT：从解码器到编码器（下游任务）openAI Transformer为我们提供了一个基于Transformer的可微调的预训练的模型。但是把LSTM换成Transformer还是让有些东西丢失了。ELMo的语言模型是双向的，而openAI Transformer则只训练一个从左到右的语言模型。那么我们能否建立一个既能从左到右预测又能从右到左预测（同时受上、下文的制约)的基于Transformer的模型呢？ MLM语言模型“我们将使用Transformer编码器”，BERT说。 “这太疯狂了”，有人说，“每个人都知道双向条件作用会让每个词在多层次的语境中间接地看到自己。” “我们将使用掩码”，BERT自信地说。 找到合适的任务来训练Transformer的编码器栈是一个复杂的问题，BERT采用了早期文献(完形填空任务)中的“带掩码的语言模型”概念来解决这个问题。 除了屏蔽15%的输入，BERT还混入一些东西，以改进模型的微调方式。有时它会随机地将一个单词替换成另一个单词，并让模型预测该位置的正确单词。 两个句子的任务如果你还记得OpenAI Transformer处理不同任务时所做的输入变换，你会注意到一些任务需要模型处理关于两个句子的信息（例如，一个句子是否是另一个句子的复述；再例如假设一个维基百科条目作为输入，一个关于这个条目的问题作为另一个输入，我们能回答这个问题吗？） 为了让BERT更好地处理多个句子之间的关系，预训练的过程还有一个额外的任务：给定两个句子（A和B）， B可能是接在A后面出现的句子吗？ 用于特征提取的BERT微调的方法并不是使用BERT的唯一方法。就像ELMo一样，你也可以使用预训练好的BERT来创建语境化的词嵌入。然后，您可以将这些嵌入表示喂给现有的模型——论文中也提到，在NER这类任务中，这种用法的最终效果也没有比用微调的方法的结果差很多。 哪种向量作为语境化嵌入的效果最好？我认为这取决于具体任务。论文比较了6中选择（与微调后的96.4分模型相比): BRET模型的创新就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。 Masked 语言模型： 而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进,掩盖的同时，要输出掩盖的词的位置，然后用真实词来预测。 Mask LM主要是为了增加模型的鲁棒性和实际性能，但是在训练时使用mask过多会影响实际任务的表现，所以做了一些处理：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题， BRET改造了一下，15%的被选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。 Next Sentence Prediction： 指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。 我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是BRET的一个创新，一般用于句级任务。 Transformer&amp;BERT总结 首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成； 第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN； 第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。 BRET最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用BRET这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。 推荐文章（由hexo文章推荐插件驱动）胶囊图神经网络BERT的前世今生]]></content>
      <categories>
        <category>论文</category>
        <category>BERT&amp;Transformer</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Next搭建个人博客]]></title>
    <url>%2F2019%2F06%2F20%2Fhexo-next%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Description这篇内容详细记述了我使用hexo+next主题搭建个人博客的过程中走过的路和跌过的坑。最终效果：个人博客 hexo环境搭建Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装可以参考hexo的官方文档的安装步骤：hexo的安装 建站其实是利用hexo的博客初始化框架进行博客站点的建立，会初始化一个新的博客文件夹，里面会包含各种博客运行所需要的包以及资源文件，建站的命令参考hexo官方文档：hexo建站 建站的过程中会出现问题，当运行完hexo init myblog质量以后会提示安装错误，但是并不要紧，紧接着通过npm install指令能够解决这个错误提示 配置紧接着是对网站进行配置，在 _config.yml 中修改大部分的配置，配置参数参考官方文档：hexo配置文档 其中要改的地方就是_config.yml里面的 new_post_name字段，将字段的值改成new_post_name: :year-:month-:day-:title.md # File name of new posts，这样在新建一条博客内容的时候，在source文件夹中就会生成一个同具有时间+名称的资源文件。 指令hexo的常用指令，包含增删查改一条博客的方法、博客静态网页的生成、静态网页的部署，动态网页的清理，清理的指令：清理的指令 hexo的工作顺序是：编辑内容，生成博文，部署博文到服务器上面。编辑内容就是利用markdown语法写号一些文章，然后将写好的文章生成css连接，生成的整个网站的代码都在public文件夹里面，部署的时候会生成一个.deploy文件夹，主要是用来进行部分，一般静态托管的站点很多，推荐的有githubpage，codepage等等。 每一次安装新的包都需要重新生成静态网页，新增一篇文章不需要重新生成配置：部署的文档 个人的hexo配置文件下面是个人的config文件的全部内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Double Killsubtitle: 深度学习description: 热爱深度学习，喜欢分析热门数据挖掘、nlp文章，平时可以约车旅游keywords: 深度学习、数据挖掘、nlpauthor: 塘朗爬坡王language:timezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :year-:month-:day-:title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/#theme: landscapetheme: next# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/univeryinli/univeryinli.github.io branch: master message: 'has been update!' ####symbols_count_time: symbols: true time: true total_symbols: true total_time: true exclude_codeblock: false### 这个是优化静态网页的加在速度的，但是经常会出现丢失动画效果，所以这个谨慎使用filter_optimize: enable: false # remove static resource query string # - like `?v=1.0.0` remove_query_string: true # remove the surrounding comments in each of the bundled files remove_comments: false css: enable: true # bundle loaded css file into the one bundle: true # use a script block to load css elements dynamically delivery: true # make specific css content inline into the html page # - only support the full path # - default is ['css/main.css'] inlines: excludes: js: # bundle loaded js file into the one bundle: true excludes: # set the priority of this plugin, # lower means it will be executed first, default is 10 priority: 12###search: path: search.xml field: post format: html limit: 10000 content: true 问题通过配置文件进行功能的启用和关闭的时候，一定要在开启功能的时候安装相应的包，否则就会出现网页加载不出来的情况，也建议前期先做出一个稳定的版本的博客版式，然后不要对博客结构和代码进行修改，这样可以使博客更加稳定，而且对插件也不要升级，除非博客时间太久远，需要更新，否则，应当以稳定为第一要务 比如上图的配置文件，filter_optimize:功能，如果只是启动这个功能，没有加入相应的插件包，那么对网站将是致命的打击，而且debug无从下手，根本不知道到底哪里出现问题。 next主题配置next主题配置相对来说复杂一些，主要依据是next官方网站推荐的文档，个人配置的思路是先重点看next下面的配置文件，然后根据配置文件的每一个目录里面的说明文档进行包的下载和配置就可以，官方文档的参考顺序和目录层级也都是根据配置文件进行配置的，现在废话不多说了，直接上手进行配置吧，先上我的个人配置文档：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214# ---------------------------------------------------------------# Theme Core Configuration Settings# See: https://theme-next.org/docs/theme-settings/# ---------------------------------------------------------------# If false, merge configs from `_data/next.yml` into default configuration (rewrite).# If true, will fully override default configuration by options from `_data/next.yml` (override). Only for NexT settings.# And if true, all config from default NexT `_config.yml` must be copied into `next.yml`. Use if you know what you are doing.# Useful if you want to comment some options from NexT `_config.yml` by `next.yml` without editing default config.override: false# Allow to cache content generation. Introduced in NexT v6.0.0.cache: enable: true# Redefine custom file paths. Introduced in NexT v6.0.2. If commented, will be used default custom file paths.# For example, you want to put your custom styles file outside theme directory in root `source/_data`, set `styles: source/_data/styles.styl`#custom_file_path: # Default paths: layout/_custom/* #head: source/_data/head.swig #header: source/_data/header.swig #sidebar: source/_data/sidebar.swig # Default path: source/css/_variables/custom.styl #variables: source/_data/variables.styl # Default path: source/css/_mixins/custom.styl #mixins: source/_data/mixins.styl # Default path: source/css/_custom/custom.styl #styles: source/_data/styles.styl# ---------------------------------------------------------------# Site Information Settings# See: https://theme-next.org/docs/getting-started/# ---------------------------------------------------------------favicon: small: /images/favicon-16x16-next.png medium: /images/favicon-32x32-next.png apple_touch_icon: /images/apple-touch-icon-next.png safari_pinned_tab: /images/logo.svg android_manifest: /images/manifest.json ms_browserconfig: /images/browserconfig.xml# Set rss to false to disable feed link.# Leave rss as blank to use site's feed link, and install dependencies hexo-generator-feed by `npm install hexo-generator-feed --save`.# Set rss to specific value if you have burned your feed already.rss:footer: # Specify the date when the site was setup. If not defined, current year will be used. #since: 2015 # Icon between year and copyright info. icon: # Icon name in fontawesome, see: https://fontawesome.com/v4.7.0/icons/ # `heart` is recommended with animation in red (#ff0000). name: user # If you want to animate the icon, set it to true. animated: false # Change the color of icon, using Hex Code. color: "#808080" # If not defined, `author` from Hexo main config will be used. copyright: powered: # Hexo link (Powered by Hexo). enable: true # Version info of Hexo after Hexo link (vX.X.X). version: true theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: true # Version info of NexT after scheme info (vX.X.X). version: true # Beian icp information for Chinese users. In China, every legal website should have a beian icp in website footer. # http://www.beian.miit.gov.cn beian: enable: false icp: # Any custom text can be defined here. #custom_text: Hosted by &lt;a href="https://pages.coding.me" class="theme-link" rel="noopener" target="_blank"&gt;Coding Pages&lt;/a&gt;# Creative Commons 4.0 International License.# See: https://creativecommons.org/share-your-work/licensing-types-examples# Available values of license: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero# You can set a language value if you prefer a translated version of CC license.# CC licenses are available in 39 languages, where you can find the specific and correct abbreviation you need.# Valid values of language: deed.zh, deed.fr, deed.de, etc.creative_commons: license: by-nc-sa sidebar: false post: false language:# `Follow me on GitHub` banner in the top-right corner.github_banner: enable: true permalink: https://github.com/univeryinli title: Follow me on GitHub# ---------------------------------------------------------------# SEO Settings# ---------------------------------------------------------------# Disable Baidu transformation on mobile devices.disable_baidu_transformation: false# Set a canonical link tag in your hexo, you could use it for your SEO of blog.# See: https://support.google.com/webmasters/answer/139066# Tips: Before you open this tag, remember set up your URL in hexo _config.yml (e.g. url: http://yourdomain.com)canonical: true# Change headers hierarchy on site-subtitle (will be main site description) and on all post / page titles for better SEO-optimization.seo: true# If true, will add site-subtitle to index page, added in main hexo config.# subtitle: Subtitleindex_with_subtitle: false# Automatically add external URL with BASE64 encrypt &amp; decrypt.exturl: false# Google Webmaster tools verification.# See: https://www.google.com/webmasters#google_site_verification:# Bing Webmaster tools verification.# See: https://www.bing.com/webmaster#bing_site_verification:# Yandex Webmaster tools verification.# See: https://webmaster.yandex.ru#yandex_site_verification:# Baidu Webmaster tools verification.# See: https://ziyuan.baidu.com/site#baidu_site_verification:# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEO.baidu_push: true# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash from link value (/archives -&gt; archives).# Usage: `Key: /link/ || icon`# Key is the name of menu item. If the translation for this item is available, the translated text will be loaded, otherwise the Key name will be used. Key is case-senstive.# Value before `||` delimiter is the target link.# Value after `||` delimiter is the name of FontAwesome icon. If icon (with or without delimiter) is not specified, question icon will be loaded.# External url should start with http:// or https://menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat# Enable / Disable menu icons / item badges.menu_settings: icons: true badges: false# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini# ---------------------------------------------------------------# Sidebar Settings# See: https://theme-next.org/docs/theme-settings/sidebar# ---------------------------------------------------------------# Posts / Categories / Tags in sidebar.site_state: true# Social Links# Usage: `Key: permalink || icon`# Key is the link label showing to end users.# Value before `||` delimiter is the target permalink.# Value after `||` delimiter is the name of FontAwesome icon. If icon (with or without delimiter) is not specified, globe icon will be loaded.social: #GitHub: https://github.com/yourname || github #E-Mail: mailto:yourname@gmail.com || envelope #Weibo: https://weibo.com/yourname || weibo #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skypesocial_icons: enable: true icons_only: false transition: false# Blog rollslinks_icon: linklinks_title: Linkslinks_layout: block#links_layout: inlinelinks: #Title: http://example.com# Sidebar Avataravatar: # In theme directory (source/images): /images/avatar.gif # In site directory (source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: false # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false# Table Of Contents in the Sidebartoc: enable: true # Automatically add list number to toc. number: true # If true, all words will placed on next lines if header width longer then sidebar width. wrap: false # If true, all level of TOC in a post will be displayed, rather than the activated part of it. expand_all: false # Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` in Front-matter. max_depth: 6sidebar: # Sidebar Position, available values: left | right (only for Pisces | Gemini). position: left #position: right # Manual define the sidebar width. If commented, will be default for: # Muse | Mist: 320 # Pisces | Gemini: 240 #width: 300 # Sidebar Display, available values (only for Muse | Mist): # - post expand on posts automatically. Default. # - always expand for all pages automatically. # - hide expand only when click on the sidebar toggle icon. # - remove totally remove sidebar including sidebar toggle. display: post # Sidebar offset from top menubar in pixels (only for Pisces | Gemini). offset: 12 # Enable sidebar on narrow view (only for Muse | Mist). onmobile: false # Click any blank part of the page to close sidebar (only for Muse | Mist). dimmer: falseback2top: enable: true # Back to top in sidebar. sidebar: false # Scroll percent label in b2t button. scrollpercent: false# A button to open designated chat widget in sidebar.# Firstly, you need enable the chat service you want to activate its sidebar button.chat: enable: false #service: chatra #service: tidio icon: comment # icon in Font Awesome 4, set false to disable icon text: Chat # button text, change it as you wish# ---------------------------------------------------------------# Post Settings# See: https://theme-next.org/docs/theme-settings/posts# ---------------------------------------------------------------# Set the text alignment in the posts.text_align: # Available values: start | end | left | right | center | justify | justify-all | match-parent desktop: justify mobile: justify# Automatically scroll page to section which is under &lt;!-- more --&gt; mark.scroll_to_more: true# Automatically saving scroll position on each post / page in cookies.save_scroll: false# Automatically excerpt description in homepage as preamble text.excerpt_description: true# Automatically Excerpt (Not recommend).# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150# Read more button# If true, the read more button would be displayed in excerpt section.read_more_btn: true# Post meta display settingspost_meta: item_text: true created_at: true updated_at: enable: true another_day: true categories: true# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true item_text_post: true item_text_total: false awl: 2 wpm: 275codeblock: # Manual define the border radius in codeblock, leave it blank for the default value: 1 border_radius: # Add copy button on codeblock copy_button: enable: false # Show text copy result show_result: false # Style: only 'flat' is currently available, leave it blank if you prefer default theme style:# Use icon instead of the symblo # to indicate the tag at the bottom of the posttag_icon: false# Wechat Subscriberwechat_subscriber: enable: false #qcode: /path/to/your/wechatqcode e.g. /uploads/wechat-qcode.jpg #description: e.g. subscribe to my blog by scanning my public wechat account# Reward (Donate)reward_settings: # If true, reward would be displayed in every article by default. # You can show or hide reward in a specific article throuth `reward: true | false` in Front-matter. enable: false animation: false #comment: Donate comment herereward: #wechatpay: /images/wechatpay.png #alipay: /images/alipay.png #bitcoin: /images/bitcoin.png# Related popular posts# Dependencies: https://github.com/tea3/hexo-related-popular-postsrelated_posts: enable: false title: # custom header, leave empty to use the default one display_in_home: false params: maxCount: 5 #PPMixingRate: 0.0 #isDate: false #isImage: false #isExcerpt: false# Post edit# Dependencies: https://github.com/hexojs/hexo-deployer-gitpost_edit: enable: false url: https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name # Link for view source. #url: https://github.com/user-name/repo-name/edit/branch-name/subdirectory-name # Link for fork &amp; edit.# ---------------------------------------------------------------# Misc Theme Settings# ---------------------------------------------------------------# Reduce padding / margin indents on devices with narrow width.mobile_layout_economy: false# Android Chrome header panel color ($brand-bg / $headband-bg =&gt; $black-deep).android_chrome_color: "#222"# Hide sticky headers and color the menu bar on Safari (iOS / macOS).safari_rainbow: false# Optimize the display of scrollbars on webkit based browsers.custom_scrollbar: false# Custom Logo# Do not support Scheme Mist currently.custom_logo: enable: false image: #/uploads/custom-logo.jpg# Code Highlight theme# Available values: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal# Enable "cheers" for archive page.cheers: true# TagCloud settings for tags page.tagcloud: # If true, font size, font color and amount of tags can be customized enable: true # All values below are same as default, change them by yourself min: 30 # min font size in px max: 45 # max font size in px start: "#ccc" # start color (hex, rgba, hsla or color keywords) end: "#111" # end color (hex, rgba, hsla or color keywords) amount: 200 # amount of tags, change it if you have more than 200 tags# ---------------------------------------------------------------# Font Settings. Introduced in NexT v5.0.1.# Find fonts on Google Fonts (https://www.google.com/fonts)# All fonts set here will have the following styles:# light, light italic, normal, normal italic, bold, bold italic# Be aware that setting too much fonts will cause site running slowly# ---------------------------------------------------------------# To avoid space between header and sidebar in scheme Pisces / Gemini, Web Safe fonts are recommended for `global` (and `logo`):# Arial | Tahoma | Helvetica | Times New Roman | Courier New | Verdana | Georgia | Palatino | Garamond | Comic Sans MS | Trebuchet MS# ---------------------------------------------------------------font: enable: false # Uri of fonts host, e.g. //fonts.googleapis.com (Default). host: # Font options: # `external: true` will load this font family from `host` above. # `family: Times New Roman`. Without any quotes. # `size: xx`. Use `px` as unit. # Global font settings used for all elements in &lt;body&gt;. global: external: true family: Lato size: # Font settings for Headlines (H1, H2, H3, H4, H5, H6). # Fallback to `global` font settings. headings: external: true family: size: # Font settings for posts. # Fallback to `global` font settings. posts: external: true family: # Font settings for Logo. # Fallback to `global` font settings. logo: external: true family: size: # Font settings for &lt;code&gt; and code blocks. codes: external: true family: size:# ---------------------------------------------------------------# Third Party Services Settings# See: https://theme-next.org/docs/third-party-services/# You may need to install dependencies or set CDN URLs in `vendors`# There are two different CDN providers by default:# - jsDelivr (cdn.jsdelivr.net), works everywhere even in China# - CDNJS (cdnjs.cloudflare.com), provided by cloudflare# ---------------------------------------------------------------# Math Equations Render Supportmath: enable: true # Default (true) will load mathjax / katex script on demand. # That is it only render those page which has `mathjax: true` in Front-matter. # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE. per_page: true engine: mathjax #engine: katex # hexo-renderer-pandoc (or hexo-renderer-kramed) needed to full MathJax support. mathjax: cdn: //cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML #cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML # See: https://mhchem.github.io/MathJax-mhchem/ #mhchem: //cdn.jsdelivr.net/npm/mathjax-mhchem@3 #mhchem: //cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0 # hexo-renderer-markdown-it-plus (or hexo-renderer-markdown-it with markdown-it-katex plugin) needed to full Katex support. katex: cdn: //cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css #cdn: //cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css copy_tex: # See: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex enable: false copy_tex_js: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js copy_tex_css: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css# Han Support# Dependencies: https://github.com/theme-next/theme-next-hanhan: false# Pangu Support# Dependencies: https://github.com/theme-next/theme-next-pangu# For more information: https://github.com/vinta/pangu.jspangu: false# Quicklink Support# Dependencies: https://github.com/theme-next/theme-next-quicklink# Visit https://github.com/GoogleChromeLabs/quicklink for detailsquicklink: enable: true # Quicklink (quicklink.umd.js script) is loaded on demand # Add `quicklink: true` in Front-matter of the page or post you need # Home page and archive page can be controlled through home and archive options below home: true archive: true # Default (true) will initialize quicklink after the load event fires delay: true # Custom a time in milliseconds by which the browser must execute prefetching timeout: 3000 # Default (true) will enable fetch() or falls back to XHR priority: true # For more flexibility you can add some patterns (RegExp, Function, or Array) to ignores # See: https://github.com/GoogleChromeLabs/quicklink#custom-ignore-patterns # Leave ignores as empty if you don't understand what it means # Example: # ignores: # - /\/api\/?/ # - uri =&gt; uri.includes('.xml') # - (uri, el) =&gt; el.hasAttribute('noopener') ignores:# Bookmark Support# Dependencies: https://github.com/theme-next/theme-next-bookmarkbookmark: enable: true # If auto, save the reading position when closing the page or clicking the bookmark-icon. # If manual, only save it by clicking the bookmark-icon. save: auto# Reading progress bar# Dependencies: https://github.com/theme-next/theme-next-reading-progressreading_progress: enable: true color: "#37c6c0" height: 4px# Google Calendar# Share your recent schedule to others via calendar page.# API Documentation: https://developers.google.com/google-apps/calendar/v3/reference/events/list# To get api_key: https://console.developers.google.com# Create &amp; manage a public Google calendar: https://support.google.com/calendar/answer/37083calendar: enable: false calendar_id: &lt;required&gt; # Your Google account E-Mail api_key: &lt;required&gt; orderBy: startTime offsetMax: 24 # Time Range offsetMin: 4 # Time Range showDeleted: false singleEvents: true maxResults: 250# ---------------------------------------------------------------# Comments and Widgets# See: https://theme-next.org/docs/third-party-services/comments-and-widgets# ---------------------------------------------------------------# Disqusdisqus: enable: false shortname: count: true lazyload: false# DisqusJS# Alternative Disqus - Render comment component using Disqus API# Demo: https://suka.js.org/DisqusJS/disqusjs: enable: false # API Endpoint of Disqus API (https://disqus.com/api/) # leave api empty if you are able to connect to Disqus API # otherwise you need a reverse proxy for Disqus API # For example: # api: https://disqus.skk.moe/disqus/ api: apikey: # register new application from https://disqus.com/api/applications/ shortname: # See: https://disqus.com/admin/settings/general/# Changyanchangyan: enable: false appid: appkey:# Valine# You can get your appid and appkey from https://leancloud.cn# More info available at https://valine.js.orgvaline: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: gvzxDpcfMG00bkkl9bF8DjRV-gzGzoHsz # your leancloud application appid appkey: eNWU8dxhUST7JSzUeK3uCM3T # your leancloud application appkey notify: false # mail notifier, See: https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size language: # language, available values: en, zh-cn visitor: false # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # if false, comment count will only be displayed in post page, not in home page# LiveRe comments system# You can get your uid from https://livere.com/insight/myCode (General web site)#livere_uid: your uid# Gitment# Introduction: https://github.com/imsun/gitmentgitment: enable: false mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide 'Powered by ...' on footer, and more language: # Force language, or auto switch by theme github_user: # MUST HAVE, Your Github Username github_repo: # MUST HAVE, The name of the repo you use to store Gitment comments client_id: # MUST HAVE, Github client id for the Gitment client_secret: # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled# Gitalk# Demo: https://gitalk.github.iogitalk: enable: false github_id: # Github repo owner repo: # Repository name to store issues client_id: # Github Application Client ID client_secret: # Github Application Client Secret admin_user: # GitHub repo owner and collaborators, only these guys can initialize github issues distraction_free_mode: true # Facebook-like distraction free mode # Gitalk's display language depends on user's browser or system environment # If you want everyone visiting your site to see a uniform language, you can set a force language value # Available values: en, es-ES, fr, ru, zh-CN, zh-TW language:# ---------------------------------------------------------------# Content Sharing Services# See: https://theme-next.org/docs/third-party-services/content-sharing-services# ---------------------------------------------------------------# Baidu Share# Available values: button | slide# Warning: Baidu Share does not support https.#baidushare:## type: button# AddThis Share, See: https://www.addthis.com# Go to https://www.addthis.com/dashboard to customize your tools.#add_this_id:# Likely Share# See: https://ilyabirman.net/projects/likely/# Likely supports four looks, nine social networks, any button text# You are free to modify the text value and order of any networklikely: enable: false look: normal # available values: normal, light, small, big networks: twitter: Tweet facebook: Share linkedin: Link gplus: Plus vkontakte: Share odnoklassniki: Class telegram: Send whatsapp: Send pinterest: Pin# NeedMoreShare2# Dependencies: https://github.com/theme-next/theme-next-needmoreshare2# iconStyle: default | box# boxForm: horizontal | vertical# position: top / middle / bottom + Left / Center / Right# networks:# Weibo,Wechat,Douban,QQZone,Twitter,Facebook,Linkedin,Mailto,Reddit,Delicious,StumbleUpon,Pinterest,# GooglePlus,Tumblr,GoogleBookmarks,Newsvine,Evernote,Friendfeed,Vkontakte,Odnoklassniki,Mailruneedmoreshare2: enable: true postbottom: enable: true options: iconStyle: box boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook float: enable: true options: iconStyle: box boxForm: horizontal position: middleRight networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook# ---------------------------------------------------------------# Statistics and Analytics# See: https://theme-next.org/docs/third-party-services/statistics-and-analytics# ---------------------------------------------------------------# Baidu Analytics ID#baidu_analytics:# Growingio Analytics ID# Copyright 2015-2018 GrowingIO, Inc. More info available at https://www.growingio.com#growingio_analytics: #your projectId# Google Analytics#google_analytics:# tracking_id:# localhost_ignored: true# CNZZ count#cnzz_siteid:# Application Insights# See: https://azure.microsoft.com/en-us/services/application-insights#application_insights:# Post widgets &amp; FB/VK comments settings.# ---------------------------------------------------------------# Facebook SDK Supportfacebook_sdk: enable: false app_id: #&lt;app_id&gt; fb_admin: #&lt;user_id&gt; like_button: #true webmaster: #true# Facebook comments plugin# This plugin depends on Facebook SDK.# If facebook_sdk.enable is false, Facebook comments plugin is unavailable.facebook_comments_plugin: enable: false num_of_posts: 10 # min posts num is 1 width: 100% # default width is 550px scheme: light # default scheme is light (light or dark)# VKontakte API Support# To get your AppID visit https://vk.com/editapp?act=createvkontakte_api: enable: false app_id: #&lt;app_id&gt; like: true comments: true num_of_posts: 10# Star rating support to each article.# To get your ID visit https://widgetpack.comrating: enable: false id: #&lt;app_id&gt; color: fc6423# ---------------------------------------------------------------# Show number of visitors to each article.# You can visit https://leancloud.cn to get AppID and AppKey.leancloud_visitors: enable: false app_id: #&lt;app_id&gt; app_key: #&lt;app_key&gt; # Dependencies: https://github.com/theme-next/hexo-leancloud-counter-security # If you don't care about security in leancloud counter and just want to use it directly # (without hexo-leancloud-counter-security plugin), set `security` to `false`. security: true betterPerformance: false# Another tool to show number of visitors to each article.# Visit https://console.firebase.google.com/u/0/ to get apiKey and projectId.# Visit https://firebase.google.com/docs/firestore/ to get more information about firestore.firestore: enable: false collection: articles #required, a string collection name to access firestore database apiKey: #required projectId: #required bluebird: false #enable this if you want to include bluebird 3.5.1(core version) Promise polyfill# Show Views / Visitors of the website / page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: false total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: true post_views_icon: eye# Tencent analytics ID#tencent_analytics:# Tencent MTA ID#tencent_mta:# ---------------------------------------------------------------# Search Services# See: https://theme-next.org/docs/third-party-services/search-services# ---------------------------------------------------------------# Algolia Search# See: https://theme-next.org/docs/third-party-services/search-services#Algolia-Search# Dependencies: https://github.com/theme-next/theme-next-algolia-instant-searchalgolia_search: enable: false hits: per_page: 10 labels: input_placeholder: Search for Posts hits_empty: "We didn't find any results for the search: $&#123;query&#125;" hits_stats: "$&#123;hits&#125; results found in $&#123;time&#125; ms"# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # If auto, trigger search by changing input. # If manual, trigger search by pressing enter key or search button. trigger: auto # Show top n results per article, show all results by setting to -1 top_n_per_article: 1 # Unescape html strings to the readable one. unescape: false# Swiftype Search API Key#swiftype_key:# ---------------------------------------------------------------# Chat Services# See: https://theme-next.org/docs/third-party-services/chat-services# ---------------------------------------------------------------# Chatra Support# See: https://chatra.io# Dashboard: https://app.chatra.io/settings/generalchatra: enable: false async: true id: # visit Dashboard to get your ChatraID #embed: # unfinished experimental feature for developers, See: https://chatra.io/help/api/#injectto# Tidio Support# See: https://www.tidiochat.com# Dashboard: https://www.tidiochat.com/panel/dashboardtidio: enable: false key: # Public Key, get it from Dashboard, See: https://www.tidiochat.com/panel/settings/developer# ---------------------------------------------------------------# Tags Settings# See: https://theme-next.org/docs/tag-plugins/# ---------------------------------------------------------------# Note tag (bs-callout)note: # Note tag style values: # - simple bs-callout old alert style. Default. # - modern bs-callout new (v2-v3) alert style. # - flat flat callout style with background, like on Mozilla or StackOverflow. # - disabled disable all CSS styles import of note tag. style: simple icons: false border_radius: 3 # Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6). # Offset also applied to label tag variables. This option can work with disabled note tag. light_bg_offset: 0# Tabs tagtabs: enable: true transition: tabs: false labels: true border_radius: 0# PDF tag, requires two plugins: pdfObject and pdf.js# pdfObject will try to load pdf files natively, if failed, pdf.js will be used.# The following `cdn` setting is only for pdfObject, because cdn for pdf.js might be blocked by CORS policy.# So, you must install the dependency of pdf.js if you want to use pdf tag and make it available to all browsers.# See: https://github.com/theme-next/theme-next-pdfpdf: enable: true # Default height height: 500px pdfobject: cdn: //cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js #cdn: //cdnjs.cloudflare.com/ajax/libs/pdfobject/2.1.1/pdfobject.min.js# Mermaid tagmermaid: enable: false # Available themes: default | dark | forest | neutral theme: forest cdn: //cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js #cdn: //cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js# ---------------------------------------------------------------# Animation Settings# ---------------------------------------------------------------# Use velocity to animate everything.motion: enable: true async: false transition: # Transition variants: # fadeIn | fadeOut | flipXIn | flipXOut | flipYIn | flipYOut | flipBounceXIn | flipBounceXOut | flipBounceYIn | flipBounceYOut # swoopIn | swoopOut | whirlIn | whirlOut | shrinkIn | shrinkOut | expandIn | expandOut # bounceIn | bounceOut | bounceUpIn | bounceUpOut | bounceDownIn | bounceDownOut | bounceLeftIn | bounceLeftOut | bounceRightIn | bounceRightOut # slideUpIn | slideUpOut | slideDownIn | slideDownOut | slideLeftIn | slideLeftOut | slideRightIn | slideRightOut # slideUpBigIn | slideUpBigOut | slideDownBigIn | slideDownBigOut | slideLeftBigIn | slideLeftBigOut | slideRightBigIn | slideRightBigOut # perspectiveUpIn | perspectiveUpOut | perspectiveDownIn | perspectiveDownOut | perspectiveLeftIn | perspectiveLeftOut | perspectiveRightIn | perspectiveRightOut post_block: fadeIn post_header: slideDownIn post_body: slideDownIn coll_header: slideLeftIn # Only for Pisces | Gemini. sidebar: slideUpIn# Fancybox. There is support for old version 2 and new version 3.# Choose only one variant, do not need to install both.# To install 2.x: https://github.com/theme-next/theme-next-fancybox# To install 3.x: https://github.com/theme-next/theme-next-fancybox3fancybox: true# Polyfill to remove click delays on browsers with touch UIs.# Dependencies: https://github.com/theme-next/theme-next-fastclickfastclick: false# Vanilla JavaScript plugin for lazyloading images.# Dependencies: https://github.com/theme-next/theme-next-jquery-lazyloadlazyload: true# Progress bar in the top during page loading.# Dependencies: https://github.com/theme-next/theme-next-pacepace: false# Themes list:# pace-theme-big-counter | pace-theme-bounce | pace-theme-barber-shop | pace-theme-center-atom# pace-theme-center-circle | pace-theme-center-radar | pace-theme-center-simple | pace-theme-corner-indicator# pace-theme-fill-left | pace-theme-flash | pace-theme-loading-bar | pace-theme-mac-osx | pace-theme-minimalpace_theme: pace-theme-minimal# Canvas-nest# Dependencies: https://github.com/theme-next/theme-next-canvas-nestcanvas_nest: enable: true onmobile: true # display on mobile or not color: "0,0,255" # RGB values, use ',' to separate opacity: 0.5 # the opacity of line: 0~1 zIndex: -1 # z-index property of the background count: 99 # the number of lines# JavaScript 3D library.# Dependencies: https://github.com/theme-next/theme-next-three# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false# Canvas-ribbon# Dependencies: https://github.com/theme-next/theme-next-canvas-ribbon# size: The width of the ribbon.# alpha: The transparency of the ribbon.# zIndex: The display level of the ribbon.canvas_ribbon: enable: false size: 300 alpha: 0.6 zIndex: -1#! ---------------------------------------------------------------#! DO NOT EDIT THE FOLLOWING SETTINGS#! UNLESS YOU KNOW WHAT YOU ARE DOING#! See: https://theme-next.org/docs/advanced-settings#! ---------------------------------------------------------------# Script Vendors. Set a CDN address for the vendor you want to customize.# For example# jquery: https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js# Be aware that you would better use the same version as internal ones to avoid potential problems.# Please use the https protocol of CDN files when you enable https on your site.vendors: # Internal path prefix. Please do not edit it. _internal: lib # Internal version: 3.4.1 # Example: # jquery: //cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js jquery: //cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js #jquery: # Internal version: 2.1.5 &amp; 3.5.7 # See: https://fancyapps.com/fancybox # Example: # fancybox: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js fancybox: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.js # fancybox_css: //cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css fancybox_css: //cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.6/jquery.fancybox.min.css #fancybox: #fancybox_css: # Internal version: 1.0.6 # See: https://github.com/ftlabs/fastclick # Example: # fastclick: //cdn.jsdelivr.net/npm/fastclick@1/lib/fastclick.min.js # fastclick: //cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js fastclick: # Internal version: 1.9.7 # See: https://github.com/tuupola/jquery_lazyload # Example: # lazyload: //cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js lazyload: //cdnjs.cloudflare.com/ajax/libs/jquery_lazyload/1.9.7/jquery.lazyload.min.js #lazyload: # Internal version: 1.2.1 # See: http://velocityjs.org # Example: # velocity: //cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js # velocity: //cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js # velocity_ui: //cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js # velocity_ui: //cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js velocity: velocity_ui: # Internal version: 4.7.0 # See: https://fontawesome.com # Example: # fontawesome: //cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css # fontawesome: //cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css fontawesome: # Internal version: 2.10.4 # See: https://www.algolia.com # Example: # algolia_instant_js: //cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.js # algolia_instant_css: //cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.css algolia_instant_js: algolia_instant_css: # Internal version: 1.0.2 # See: https://github.com/HubSpot/pace # Example: # pace: //cdn.jsdelivr.net/npm/pace-js@1/pace.min.js # pace: //cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js # pace_css: //cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css # pace_css: //cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css pace: pace_css: # Internal version: 1.0.0 # See: https://github.com/theme-next/theme-next-canvas-nest # Example: canvas_nest: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest.min.js canvas_nest_nomobile: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@1/canvas-nest-nomobile.min.js #canvas_nest: #canvas_nest_nomobile: # Internal version: 1.0.0 # See: https://github.com/theme-next/theme-next-three # Example: #three: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js #three_waves: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three-waves.min.js # canvas_lines: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_lines.min.js # canvas_sphere: //cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/canvas_sphere.min.js three: three_waves: canvas_lines: canvas_sphere: # Internal version: 1.0.0 # See: https://github.com/zproo/canvas-ribbon # Example: # canvas_ribbon: //cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js canvas_ribbon: # Internal version: 3.3.0 # See: https://github.com/ethantw/Han # Example: # han: //cdn.jsdelivr.net/npm/han-css@3/dist/han.min.css # han: //cdnjs.cloudflare.com/ajax/libs/Han/3.3.0/han.min.css han: # Internal version: 4.0.7 # See: https://github.com/vinta/pangu.js # Example: # pangu: //cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js # pangu: //cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js pangu: # Internal version: 1.0.0 # See: https://github.com/GoogleChromeLabs/quicklink # Example: quicklink: //cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js #quicklink: # Internal version: 1.0.0 # See: https://github.com/revir/need-more-share2 # Example: needmoreshare2_js: //cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js needmoreshare2_css: //cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css #needmoreshare2_js: #needmoreshare2_css: # Internal version: 1.0.0 # See: https://github.com/theme-next/theme-next-bookmark # Example: bookmark: //cdn.jsdelivr.net/gh/theme-next/theme-next-bookmark@1/bookmark.min.js #bookmark: # Internal version: 1.1 # See: https://github.com/theme-next/theme-next-reading-progress # Example: reading_progress: //cdn.jsdelivr.net/gh/theme-next/theme-next-reading-progress@1/reading_progress.min.js #reading_progress: # leancloud-storage # See: https://www.npmjs.com/package/leancloud-storage # Example: # leancloud: //cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js leancloud: # valine # See: https://github.com/xCss/Valine # Example: # valine: //cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js valine: //cdnjs.cloudflare.com/ajax/libs/valine/1.3.4/Valine.min.js #valine: # gitalk &amp; js-md5 # See: https://github.com/gitalk/gitalk, https://github.com/emn178/js-md5 # Example: # gitalk_js: //cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js # gitalk_css: //cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css # md5: //cdn.jsdelivr.net/npm/js-md5@0/src/md5.min.js gitalk_js: gitalk_css: md5: # likely # See: https://github.com/ilyabirman/Likely # Example: # likely_js: //cdn.jsdelivr.net/npm/ilyabirman-likely@2/release/likely.js # likely_css: //cdn.jsdelivr.net/npm/ilyabirman-likely@2/release/likely.css likely_js: likely_css: # DisqusJS # See: https://github.com/SukkaW/DisqusJS # Example: # disqusjs_js: //cdn.jsdelivr.net/npm/disqusjs@1/dist/disqus.js # disqusjs_css: //cdn.jsdelivr.net/npm/disqusjs@1/dist/disqusjs.css disqusjs_js: disqusjs_css:# Assetscss: cssjs: jsimages: images 根据官方文档修改配置通过配置文档就能看到这个next主题所实现的功能，每一个功能块都在配置文档里面分块标识出来了，并且每一个大功能块里面的子功能的参考文档也都有详细的安装说明，下面举例子说明 下面是一个功能块的配置文档图，我们可以看到大功能块的配置文档说明：post功能模块的说明文档，里面有整个post模块的详细说明以及功能介绍 接着可以看到这个功能模块的子功能的开启按钮和关闭按钮，# Post wordcount display settings，这是一个对每篇文章进行计数的子功能，会显示在文档的最上角，该子功能模块给了一个安装说明文档，# Dependencies: https://github.com/theme-next/hexo-symbols-count-time，根据地址找到文档，并且按照文档安装该功能所需的插件或者npm包，然后开启该功能就可以 其他的功能也都是一样的，有的子功能可能不需要安装插件就可以打开，直接在配置文档里面就可以进行打开，此时参考大功能模块的参考文档就可以，每一个大功能都有介绍，但是开启子功能的时候碰到文档的，一定要耐心看完文档，可以避免走很多弯路，其实这种博客搭建起来很简单，就是完全按照配置文档配置就好了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# ---------------------------------------------------------------# Post Settings# See: https://theme-next.org/docs/theme-settings/posts# ---------------------------------------------------------------# Set the text alignment in the posts.text_align: # Available values: start | end | left | right | center | justify | justify-all | match-parent desktop: justify mobile: justify# Automatically scroll page to section which is under &lt;!-- more --&gt; mark.scroll_to_more: true# Automatically saving scroll position on each post / page in cookies.save_scroll: false# Automatically excerpt description in homepage as preamble text.excerpt_description: true# Automatically Excerpt (Not recommend).# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150# Read more button# If true, the read more button would be displayed in excerpt section.read_more_btn: true# Post meta display settingspost_meta: item_text: true created_at: true updated_at: enable: true another_day: true categories: true# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true item_text_post: true item_text_total: false awl: 2 wpm: 275codeblock: # Manual define the border radius in codeblock, leave it blank for the default value: 1 border_radius: # Add copy button on codeblock copy_button: enable: false # Show text copy result show_result: false # Style: only 'flat' is currently available, leave it blank if you prefer default theme style:# Use icon instead of the symblo # to indicate the tag at the bottom of the posttag_icon: false# Wechat Subscriberwechat_subscriber: enable: false #qcode: /path/to/your/wechatqcode e.g. /uploads/wechat-qcode.jpg #description: e.g. subscribe to my blog by scanning my public wechat account# Reward (Donate)reward_settings: # If true, reward would be displayed in every article by default. # You can show or hide reward in a specific article throuth `reward: true | false` in Front-matter. enable: false animation: false #comment: Donate comment herereward: #wechatpay: /images/wechatpay.png #alipay: /images/alipay.png #bitcoin: /images/bitcoin.png# Related popular posts# Dependencies: https://github.com/tea3/hexo-related-popular-postsrelated_posts: enable: false title: # custom header, leave empty to use the default one display_in_home: false params: maxCount: 5 #PPMixingRate: 0.0 #isDate: false #isImage: false #isExcerpt: false# Post edit# Dependencies: https://github.com/hexojs/hexo-deployer-gitpost_edit: enable: false url: https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name # Link for view source. #url: https://github.com/user-name/repo-name/edit/branch-name/subdirectory-name # Link for fork &amp; edit. next功能推荐功能推荐其实可以参考next主题的官方Github，把next所有的子项目都配置一遍就可以了。 next的GitHub：https://github.com/theme-next next官方说明文档：https://theme-next.org/docs/ schem:主题里面还有副主题，选择一个副主题，schem字段里面的内容 阅读进度条功能 本地搜索功能 canvas-nest功能，可以生成背景效果 右上角GitHub按钮功能 右下角的一键回到顶端功能 书签功能，可以收藏功能看到的网页 vasline的评论功能，参考文档就可以配置完成，过程中不要怕麻烦，一劳永逸的事情 fancy-box图片显示功能，可以将图片进行个性化显示，如果不安装图片注释将无法使用 左下角的分享功能也是必须的，所以也要进行配置 左边菜单栏配置菜单栏配置不能通过配置文档来实现，菜单栏其实是需要新建一个网页，然后来配置菜单栏的子菜单的，比如添加about子菜单，需要在source文件下新建一个about.md的文档，然后在next的menu配置里面填写about: /source/about/index.md的地址，这样该菜单栏就会生效，想要这个子菜单有什么样的效果，直接写markdown就可以，这就是hexo的精髓所在，在这个框架下，任何markdown语言写出来的文本都可以渲染，访问的路径就是source文档里面的路径加上服务器的ip地址就可以 我新建的about文件的在source里面的地址为，about: /source/about/index.md，那么我的网页访问地址为，https://univeryinli.github.io/about，会直接对index进行渲染，相当于一个子页面，也可以写多级的index，可以组成一个庞大的多级菜单栏 总结hexo非常适用个人博客和站点的建立，只需要很少的维护成本和极少的代码编写就可以实现一个比较酷炫的个人博客，可以在菜单栏中配置多个标签，让个人博客成为一个个人站点也都是可以的，配置好网站后就不要升级网站的基本代码了，不然会出现很多问题就麻烦了。 配置完网页以后，剩下的就交给markdown，学会写markdown将会使你的写作水平更上一个台阶，它为说明文档而生，而且科研领域里面用得比较多的ctex也跟这个语言有异曲同工之妙，非常好用，层次分明，逻辑清洗，合理利用多级标题、有序列表、无序列表将会使你的工作得到充分的展示。 推荐文章（由hexo文章推荐插件驱动）Twitter プラグインを使用しないことにしました]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu k80深度学习环境搭建]]></title>
    <url>%2F2019%2F05%2F27%2FUbuntu-k80%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Description本文讲述深度学习环境的配置，K80的配置，解决Linux下有界面的无线循环问题！ 英伟达驱动安装 英伟达驱动下载：https://www.nvidia.cn/Download/driverResults.aspx/135493/cn/ 由于是驱动的冲突，那么自然是要杀掉和显卡结合不是那么紧密的草根板驱动nouveau了，加入黑名单是我们要做的第一件事，这样启动以后就不会默认使用草根驱动； 12345678910111213cd /etc/modprobe.d/# 文件夹下创建touch blacklist-nouveau.confvim blacklist-nouveau.confblacklist-nouveau.conf 中加入黑名单blacklist nouveauoptions nouveau modeset=0# 更新的blacklistupdate initramfs -u命令得到# 重启系统，强力保证blacklist生效reboot# 查看是否vouveau真的被禁止掉了，如果没有任何内容出现，那么草根驱动被禁止掉了lsmod | grep nouveau 下载NVIDIA官方的K80显卡驱动，一般驱动都是通过deb包进行安装，但是安装后会加入OpenGL的驱动，所以必须得使用.run的文件，.run文件下载地址：https://www.nvidia.cn/Download/driverResults.aspx/135493/cn/ 如果不用.run方式的话，那么就会进入Linux的无限循环界面。1./XXX-NVIDIA.run --no-opengl-files (重要的事情说三遍，这里面的-和字母之间没有空格、这里面的-和字母之间没有空格、这里面的‘-’和字母之间没有空格) 出现蓝色的背景界面，如果出现了（X server is running的现象，要注意用户态输入sudo service lightdm stop关闭桌面管理器 ），然后accept协议，接着出现the distribution provided pre-install scripts failed的提示，忽视它，然后继续安装下去，一路OK然后reboot系统，最终得到完整的gnome桌面系统。1sudo service lightdm stop 检验是否安装成功，在命令行界面下输入 nvidia-smi检验是否安装成功1nvidia-smi 安装源管理软件包Anaconda：下载地址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.2.0-Linux-x86_64.sh/ Cuda安装： 下载CUDAhttps://developer.nvidia.com/cuda-downloads/ 安装cuda 123sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.debsudo apt updatesudo apt -y install cuda 将CUDA路径添加至环境变量在终端输入 1sudo gedit /etc/profile 在profile文件中添加：123export CUDA_HOME=/usr/local/cuda-8.0export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; source /etc/profile即可1source /etc/profile 验证安装成功：1nvcc -V 会得到相应的nvcc编译器相应的信息，那么CUDA配置成功了。(记得重启系统) 如果要进行cuda性能测试，可以进行：12cd /usr/local/cuda/samplessudo make -j8 编译完成后，可以进samples/bin/…/…/…的底层目录，运行各类实例。 安装tensorflow 官方连接：https://www.tensorflow.org/install/install_linux/ 参考官方文档的pip源部分:] pip安装的时候千万注意：sudo pip3 install –upgrade 后面的接的gpu版本的连接，在官网文档最后面，python务必与对应的tensorflow版本对应。 安装完后也要注意依赖库版本的修复，因为开源代码，版本库版本特别多，所以如果有版本不兼容，那么一定要进行修复，如何修复自行百度。 tensorflow验证：1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello)) 安装keras： 安装keras： 1sudo pip install -U --pre keras 安装完毕后，输入python，然后输入： 123 import tensorflow import keras Keras中mnist数据集测试 123git clone https://github.com/fchollet/keras.gitcd keras/examples/ python mnist_mlp.py 程序无错进行，至此，keras安装完成。 推荐文章（由hexo文章推荐插件驱动）Ubuntu k80深度学习环境搭建]]></content>
      <categories>
        <category>深度学习软件</category>
      </categories>
      <tags>
        <tag>神经网络环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[胶囊图神经网络]]></title>
    <url>%2F2019%2F05%2F27%2F%E8%83%B6%E5%9B%8A%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[Description本文讲述胶囊神经网络和图模型，以及胶囊图模型的具体结构和胶囊图模型的具体应用，最后给出了参考文献！ 胶囊网络（CapsNet）卷积网络（CNN）的目标识别 卷积神经网络首先学会识别边界和颜色，然后将这些信息用于识别形状和图形等更复杂的实体。比如在人脸识别上，他们学会从眼睛和嘴巴开始识别最终到整个面孔，最后根据脸部形状特征识别出是不是人的脸。 卷积网络对不同人脸的识别 深度卷积网络的缺陷 CNN 对物体之间的空间关系 (spatial relationship) 的识别能力不强，比如上图中的嘴巴和眼睛换位置了还被识别成人 CNN 对物体旋转之后的识别能力不强 (微微旋转还可以)，比如卡戴珊倒过来就被识别成头发了 基于以上缺陷，Hinton在神经网络中提出capsule的概念，个人认为也可以叫做向量神经元，capsule不同于传统的标量神经元，而是向量神经元，具有方向性。 参考文献：Sabour, S., et al. (2017). Dynamic routing between capsules. Advances in neural information processing systems. 标量神经元与向量神经元标量神经元的计算： 将输入标量 x 乘上权重 w 对加权的输入标量求和成标量 a 用非线性函数将标量 a 转化成标量 h 向量神经元的计算： 将输入向量 u 用矩向量W 加工成新的输入向量 U 将输入向量 U 乘上耦合系数 C 对加权的输入向量求和成向量 s 用非线性函数将向量 s 转化成向量 v ] 胶囊的内部结构 胶囊的输入向量和输出向量计算公式 其中，v代表胶囊j的输出向量，s代表胶囊的输入向量，这个转化其实是将向量s进行一个压缩和单位化的操作。 压缩：如果s很长，那么左边项约等于1，如果s很短，那么左边项约等于0 单位化：使输出向量长度为0到1之间的向量，因此长度可以表示为特征概率值 层级胶囊的动态路由 胶囊的映射方式 u是上一个胶囊层的输出向量，w是连接两个胶囊层的向量权重，$u_{j|i}$ 是上一层的神经元经过权重后的预测向量，$c_ij$是不同层向量元的耦合系数，s为本层的胶囊向量的输入，也是预测向量的加权和，只不过权重变成了耦合系数，并不是标量元里面的权重值 动态路由系数计算 层级胶囊路由的伪代码 数字的边际损失函数 利用向量的长度来表示capsule 实体存在的概率，因此，对于数字类别k，希望高层次的capsule 能够有一个较长的输出向量，对于每个digit capsule k： 胶囊网络的结构 第一层卷积层：原始图像输入size为2828，采用大小为9的卷积核生成256个通道的特征图，并且用relu作为激活函数输出大小为25620*20的特征图 第二层卷积层（PrimaryCaps）：输入的特征图大小为2562020，每个PrimaryCaps通过8个99大小卷积核，strid为2获得，一共输出32个PrimaryCaps，即每个PrimaryCaps有66个维度8D的胶囊单元，不同的map代表不同的特征类型，同一个map中的向量代表不同的位置 第三层DigitCaps对第二层所有的胶囊进行动态路由连接到第三层，第二层一共3266个8D向量通过动态路由连接成10个16D的胶囊向量，相当于采用不同参数对第二层所有向量进行10次变维度映射。 重构重构的意思就是用预测的类别重新构建出该类别代表的实际图像。 Capsule的向量可以表征一个实例，将最后的那个正确预测类别的向量投入到后面的重构网络中，可以构建一个完整的图像，从而可以通过重建图像与原图像的欧式距离来评估此模型分类的效果，更能说明胶囊网络中向量神经元具有的优越性，图中的784即为原始图像的28*28大小。 重建作为调整方法 L代表标签，p代表预测值，r重构出的图片，左边三列是正确的结果。 重构出来的图像形状和位置和输入极其类似，这是说明胶囊网络起了作用，Capsule的确包含了物体的多个信息：特征、位置、大小等等。 后面两列是预测失败的，通过重构出来的图我们可以得到原因：3和5太像了，即使人也很难清楚的分辨出来。 胶囊网络的一些应用 可以利用胶囊向量的方向性进行物体角度估计，从而可以 弥补卷积网络在这方面的缺陷 可以对重叠图像中的特征进行有效识别 利用胶囊网络进行手语的识别，胶囊网络善于提取不同物体的角度信息，从而可以分辨不同角度的手势，为自动手语翻译提供很好的帮助 胶囊网络推荐文章 Sabour S, Frosst N, Hinton G E. Dynamic routing between capsules[C]//Advances in neural information processing systems. 2017: 3856-3866. Hinton G E, Krizhevsky A, Wang S D. Transforming auto-encoders[C]//International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011: 44-51. Hinton G. Taking inverse graphics seriously[J]. 2013. 图神经网络（Graph Neural Network）图模型的相关概念 图是一种非结构化数据，作为一种非欧几里得形数据，图分析被应用到节点分类、链路预测和聚类等方向。图网络是一种基于图域分析的深度学习方法。 图表示（graph embedding）是一种图的知识表示方法，即如何将图中的节点、边和子图以低维向量的形式表现出来。受启发于表示学习（representation learning）和词嵌入（word embedding），图嵌入技术得到了长足的发展，例&gt;如DeepWalk模型。 图神经网络的分类不同的文献对图神经网络有不一样的分类，不过基本都是根据各自的特性来进行分类的 文献1根据图模型结构：Directed Graphs、Heterogeneous Graphs、Graphs with Edge InformationZhou J, Cui G, Zhang Z, et al. Graph neural networks: A review of methods and applications[J]. arXiv preprint arXiv:1812.08434, 2018. 文献2根据聚合函数：graph convolution networks、graph attention networks、graph auto-encoders、graph generative networks、graph spatial-temporal networksWu Z, Pan S, Chen F, et al. A comprehensive survey on graph neural networks[J]. arXiv preprint arXiv:1901.00596, 2019. 图模型结构 在图中，每个节点的定义是由该节点的特征和相关节点来共同表示的。GNN的目标是训练出state embedding函数hv，该函数包含了每个节点的邻域信息$h_v$是节点v的向量化表示，可以用来预测该节点的输出$o_v$(例如节点的标签)，$x_v$是节点v的特征表示，$x_c_0$是节点v相关的边的特征表示，$h_ne$节点v相关的当前状态，$x_ne$是节点v邻接节点的特征表示，f是local transition function，被所有节点共享，根据领域信息更新当前节点装填 g被称作local output function，用来产生节点输出，实现分类或者回归任务 图模型损失函数p代表图中所有有监督节点的数量。优化是基于梯度下降算法的，并且被表示如下： H进行迭代更新，直到T时间步 权重W的梯度通过loss计算得出 进行梯度更新 图模型类型（Graph Types） 有向图（Directed Graphs） 传统的无向边可以看作是两个有向边组成的，表明两个节点之间存在着关系。然而，有向边相对与无向边来说能够表达更为丰富的信息 异构图（Heterogeneous Graphs） 包含有不同类型的几种节点，表现不同类型节点最简单的形式是，将类型用one-hot向量表示，然后与原始节点的特征向量进行拼接 Graph Inception模型将metapath的概念引入到了异构图的传播中。我们可以对邻近节点进行分类，根据其节点的类型和其距离。对于每个邻近节点群，Graph Inception将它作为一个同构图中的子图，并将来自于不同同构图的传&gt;播结果连接起来视为一个集合节点来表示 带有边信息的图（Graphs with Edge Information） 在图中，边也蕴含着丰富的信息，例如权重和边的类型等。有两种表示图的方式： 两个节点之间的边切割开成两条边，然后将边也转化成节点 在传播过程中，不同的边上有不同的权值矩阵 传播类型（Propagation Types） 在模型中，信息的传播步骤和输出步骤是获得节点或者边隐含状态的关键步骤。不同变种的GNN的聚合函数（用来聚合图中所有点的邻域信息，产生一个全局性的输出）和节点状态更新函数如下图所示，其中常见的就是卷积聚合函数和注意力聚合函数： Attention：GAT是一种注意力图网络，它将注意力机制融入到了图传播的步骤中。GAT计算每个节点的隐藏状态，通过将 “attention” 机制应用到邻近节点上，从而可以通过不同的关注力对邻接节点信息进行聚合 除此之外还有Gate和Skip connection传播类型，各自有不同的功能 训练方法（Training Methods）论文中还介绍了几种GNN的训练方法。比如Graph SAGE从节点的部分邻域聚合信息。Fast GCN采用样本采样方法替代节点的所有邻域信息。其宗旨都是为了改进图模型计算复杂这一缺点，这些训练方法，都使得模型的训练效率更加的高效，随机丢弃一些邻域点还能使图模型的鲁棒性增 应用（APPLICATIONS）图网络被广泛的应用于包括监督学习、半监督学习、无监督学习和强化学习等方向。论文中从三个不同的场景来分别阐述图网络的应用。 结构化场景：数据包含有很明确的关系结构，如物理系统、分子结构和知识图谱。 非结构化场景：数据不包含明确的关系结构，例如文本和图像等领域。 其他应用场景：例如生成式模型和组合优化模型。 各个领域图网络的应用细节如下图所示： 推荐文献 Kipf, T. N. and M. Welling (2016) Semi-Supervised Classification with Graph Convolutional Networks. ArXiv e-prints Scarselli, F., et al. (2009). “The Graph Neural Network Model.” IEEE Transactions on Neural Networks 20(1): 61-80. Veličković, P., et al. (2017) Graph Attention Networks. ArXiv e-prints Ying, R., et al. (2018) Graph Convolutional Neural Networks for Web-Scale Recommender Systems. ArXiv e-prints Bruna, J., et al. (2013) Spectral Networks and Locally Connected Networks on Graphs. ArXiv e-prints 胶囊图神经网络（Capsule Graph Neural Network）图卷积网络（GRAPH CONVOLULTIONAL NETWORK） T是信息变换矩阵，是通过邻接矩阵变换得来，大小N*N，w是节点向量维度变换矩阵 CapsGNN模型结构 Block1：基本节点胶囊提取模块，利用GCN原理生成当前时刻节点状态，作为胶囊向量，向量长度为N Block2：高级图胶囊提取模块，融合了注意力模块和动态路由，以生成多个图胶囊 Block3：图分类模块，再次利用动态路由，生成用于图分类的类胶囊 基本节点胶囊提取模块 高级图胶囊提取模块注意力机制的目的：让模型更加注重图中更重要相关的节点或者邻域信息将初始胶囊的每一个节点的所有通道的d维向量进行concat，生成第二个图的向量，利用注意力函数F计算出注意力值矩阵，然后对每一行进行归一化，得到每个通道不同节点的注意力值的归一化值，然后与原来初始胶囊相乘，可以得到&gt;带有不同注意力的胶囊向量组 公式如下：计算投票：对注意力机制生成的胶囊向量矩阵进行投票计算，其实是用一个可训练的权重矩阵对整个胶囊向量进行多次计算，最后生成P个图胶囊，每一个图胶囊都是对整个图的不同角度的观察，然后利用路由机制生成下一层的图胶囊。 图分类模块运用动态路由思想，继续将P个图胶囊路由成C个类别的分类胶囊利用分类损失函数进行梯度下降，训练整个模型 重建重建损失，将分类好的胶囊向量通过多层全连接可以重建原数据，如果是图像，那么可以重建图像，并且根据重建损失来进行全连接的训练，其目的在于可以对最终分类的效果进行可视化，而不是简单给出一个概率值，有利于模型的整&gt;体评估。 结果 提升了图模型的分类正确率 提升了图模型的效率，用更少的节点可以描绘更多的信息 能够通过图胶囊捕捉到更多图模型的属性信息 参考文献 Zhang Xinyi, L. C. (2019). “Capsule Graph Neural Network.” 演讲ppt下载参考ppt下载：图模型ppt 推荐文章（由hexo文章推荐插件驱动）BERT的前世今生BERT的前世今生]]></content>
      <categories>
        <category>论文</category>
        <category>图模型分析</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>图模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[培养专利意识]]></title>
    <url>%2F2019%2F05%2F27%2F%E5%9F%B9%E5%85%BB%E4%B8%93%E5%88%A9%E6%84%8F%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Description专利意识是指发明人或设计人对自己发明或设计的产品依法申请登记，请求国家授予《专利证书》，以取得专利独占权，保护自己劳动成果的法律意识。 培养专利意识进行专利培训工作，首先要进行“专利意识”的培训。解决好思想问题，才能产生行动能力。专利意识的内涵至少包括： 技术保护意识，科技型企业进行技术创新研发是必然的工作内容，但是并非所有的工作人员都能自觉想到对研发成果进行保护，所以我们要改变只搞研发，不注重保护，要将研发和知识产权保护这两件事情同时进行。 申请优先意识，根据专利的基本原则，以申请日作为专利申请后续审查的重要时间界点，因此在团队里面开始公布源代码或者发布会等之前，最好完成自己成果的专利申请，这样可以防止自己的专利成果与其他人的专利之间的冲突，作为技术人员应该提早谋划、应该尽早给出自己的专利撰写方案。 专利无形资产意识，技术人员一般更加关心的是技术问题，至于专利属于公司还是个人，对这个其实并不太重视，一般认为技术只是一种单纯的工具而已，用来解决公司某些应用方面的一些关键技术问题，技术问题一旦开发完成后，对技术本身这一无形资产属性其实并不太重视，只认为如机器、设备、厂房、土地是企业资产，这对公司和个人利益的保护都不好，所以技术人员在开发的时候要有专利无形资产意识。 高质量专利 专利质量是指专利权的实际法律效力 依法享受的保护范围适当 专利权稳定 难以被规避 侵权者难以逃脱法律责任 难以被发现法律保护漏洞 可阻止相同发明构思的其他实施也成专利 专利质量评价 明白标准，是指专利申请文件只介绍清楚了发明的内容，不一定能够得到授权，难以获得有效的法律保护； 授权标准，是指专利申请文件符合授权条件，单经验丰富的律师可发现保护范围内的瑕疵，导致为专利抗辩成功； 胜诉标准，是指专利申请文件没有与前人相同的撰写失误，构成专利侵权要件后就难以被抗辩。 三个层次依次进行。明白标准是指专利申请文件只介绍清楚发明的内容，授权标准是指专利。 高质量专利提高质量方法 以主张权利为重点，而不是保护产品本身； 建立专利家族；撰写有针对性的权力要求； 提交大量的在先技术； 避免纸上专利确保容易证明侵权 专利信息检索 专利技术主题检索，形成可供参考的所属技术领域的相关专利文献基础数据。 专利引文检索，从专利引文角度扩展相关参考文献的范围 专利相关人检索，从专利相关人角度扩展相关参考文献的范围 同族专利检索，去除基础数据中的重复信息 专利情报分析 申请趋势分析，获得技术发展历程及具体周期，预测未来技术发展趋势； 技术功效矩阵分析，通过技术主题内容和主要技术功能效果的研究，寻找出技术空白点和技术热点以及突破点，规避技术雷区，发现潜在的研发方向，获得创新灵感； 专利技术对比分析，与新出现的研发方向雷同的专利比较，判断相似度，以保证研发方向的创新性 专利文献阅读意识和能力在如今这瞬息万变的时代，“闭门造车”已属不可能。专利文献揭示了百分之九十以上的新的技术和前沿的技术、新的思想，我们只有在大量阅读同行业、同领域内的新的、前沿的技术专利文献，以及竞争对手的专利文献，我们才能够找到我们正确的方向，不然闭门造车半天，别人其实已经对我们研究的技术提出了一定的解决方法，这样我们会浪费很多时间和精力。 同时，我们应该着重培养研发人员如何检索与自身研发领域相关的专利文献，如何阅读专利文献，如何养成阅读习惯和增强阅读能力。我们在研发过程中必然会遭遇到各种技术难题，针对不同的技术难点，通过文献检索去寻找已有的解决方法是一个比较好的途径，可以节省一部分方案设计的时间，对于不能通过文献检索解决的问题，我们就应该提出自己的解决方法，在已有的比较优良的方法上，将技术细化为技术交底材料，以便为专利的撰写提供有力保障。 专利检索技术 PSS专利检索系统的介绍与使用 专利技术角度检索 专利技术角度检索步骤 确定技术角度检索种类 分析检索主题或方案 找出检索要素表达 填写检索要素表 选择检索系统并检索 专利检索质量评价 培养专利撰写能力发明创造要用专业的方式书写专利申请文件，该工作通常会有专利代理人来完成，所以技术人员应该提供技术交底书出来，一份好的技术交底书，有助于专利代理人撰写一份优秀的申请文件。 比较完善规整的撰写技术交底书益处多多： 可梳理思路，当技术人员在书写技术方案的同时也是检验自己技术细节是否完善的重要一步，因为我们写专利的时候往往对某一问题的解决方法都是停留在大脑中的，往往只是一个大概的思路，然后在用文字描述的情况下，恰好可理顺解决方案的思路，又可以细化我们的专利采用的技术，使我们的大脑中的想法更加具体，同时又可以检验我们大脑中想法的可行性。有时候，大脑中的想法，是不完善的，是片面的，对某一问题的认识是不清晰的，产生出来的方案也就不清晰，用文字形成技术方案可以检验我们方案的可行性，同时可以对我们方案进行补充，让我们写专利的时候的思路会更加清晰。 可以将企业的技术文档化，便于后面的研发人员的查询和使用，也可以利于公司的专利技术的管理，在以后的专利维权中也会是一种辅助性的证据。]]></content>
      <categories>
        <category>专利</category>
      </categories>
      <tags>
        <tag>专利技巧</tag>
      </tags>
  </entry>
</search>
